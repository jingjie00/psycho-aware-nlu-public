{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas pickle file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "def extract(path):\n",
    "    print(os.getcwd())\n",
    "    file = open(path, 'rb')\n",
    "    object_file = pickle.load(file)\n",
    "    file.close()\n",
    "    return object_file\n",
    "\n",
    "def save_dataset(item, dir, name):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    path = dir+\"/\"+name+\".pickle\"\n",
    "    pickle.dump(item, open(path, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingjietan/anaconda3/envs/phdwork/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sadness       joy      love     anger      fear  surprise\n",
      "0  0.001000  0.995372  0.001425  0.001084  0.000497  0.000623\n",
      "1  0.059335  0.116308  0.022859  0.736262  0.049569  0.015667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"nateraw/bert-base-uncased-emotion\", return_all_scores=True, max_length=512)\n",
    "\n",
    "def convert_list_to_dict(list_of_dicts):\n",
    "    result_dict = {}\n",
    "    for item in list_of_dicts:\n",
    "        label = item.get(\"label\")\n",
    "        score = item.get(\"score\")\n",
    "        if label is not None and score is not None:\n",
    "            result_dict[label] = score\n",
    "    return result_dict\n",
    "\n",
    "def convert_emotion(sentences):\n",
    "    # Assuming classifier and convert_list_to_dict are defined elsewhere\n",
    "\n",
    "    # Initialize an empty list to store individual DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Process each sentence in the list\n",
    "    for sentence in tqdm(sentences):\n",
    "        emo = classifier(sentence)  # Assuming classifier returns emotions for a single sentence\n",
    "        out = convert_list_to_dict(emo[0])  # Assuming convert_list_to_dict processes the emotion list\n",
    "        df = pd.DataFrame(out, index=[0])\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "print(convert_emotion([\"I am happy\",\"he\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8675/8675 [49:42<00:00,  2.91it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "mbti = pd.read_csv('../../corpus/mbti.csv')\n",
    "sentence_list = mbti['posts'].tolist()\n",
    "emotion_df_list = convert_emotion(sentence_list)\n",
    "mbti_ready = pd.concat([mbti, emotion_df_list], axis=1)\n",
    "mbti_ready.head()\n",
    "save_dataset(mbti_ready, \"../../corpus/emotion-aware-personality\", \"bert-based-uncased-mbti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv('../../corpus/imdb.csv')\n",
    "imdb['sentiment'] = imdb['sentiment'].replace('positive', 1)\n",
    "imdb['sentiment'] = imdb['sentiment'].replace('negative', 0)\n",
    "sentence_list = imdb['review'].tolist()\n",
    "emotion_df_list = convert_emotion(sentence_list)\n",
    "imdb_ready = pd.concat([imdb, emotion_df_list], axis=1)\n",
    "imdb_ready.head()\n",
    "\n",
    "save_dataset(imdb_ready, \"../../corpus/emotion-aware-sentiment\", \"bert-based-uncased-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviereview = pd.read_csv('../../corpus/movie-review.csv')\n",
    "sentence_list = moviereview['content'].tolist()\n",
    "emotion_df_list = convert_emotion(sentence_list)\n",
    "moviereview_ready = pd.concat([moviereview, emotion_df_list], axis=1)\n",
    "save_dataset(moviereview_ready, \"../../corpus/emotion-aware-sentiment\", \"bert-based-uncased-moviereview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1895/1895 [05:43<00:00,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "sdcnl = pd.read_csv('../../corpus/sdcnl.csv')\n",
    "\n",
    "# merge title and selftext column together, both have string value\n",
    "sdcnl['text'] = sdcnl['title'].astype(str) + \" | \" + sdcnl['selftext'].astype(str)\n",
    "\n",
    "# drop all column except text and is_suicide column\n",
    "sdcnl = sdcnl[['text', 'is_suicide']]\n",
    "\n",
    "sdcnl.head()\n",
    "\n",
    "sentence_list = sdcnl['text'].tolist()\n",
    "emotion_df_list = convert_emotion(sentence_list)\n",
    "sdcnl_ready = pd.concat([sdcnl, emotion_df_list], axis=1)\n",
    "sdcnl_ready.head()\n",
    "save_dataset(sdcnl_ready, \"../../corpus/emotion-aware-depression\", \"bert-based-uncased-sdcnl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [23:46<00:00, 14.02it/s]\n"
     ]
    }
   ],
   "source": [
    "twitter = pd.read_csv('../../corpus/mental-health-twitter.csv')\n",
    "twitter = twitter[[\"post_text\",\"label\"]]\n",
    "sentence_list = twitter['post_text'].tolist()\n",
    "emotion_df_list = convert_emotion(sentence_list)\n",
    "twitter_ready = pd.concat([twitter, emotion_df_list], axis=1)\n",
    "twitter_ready.head()\n",
    "save_dataset(twitter_ready, \"../../corpus/emotion-aware-depression\", \"bert-based-uncased-twitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jingjietan/Desktop/PRaware/model_aware/emotion\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>posts</th>\n",
       "      <th>sadness</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>caring</th>\n",
       "      <th>neutral</th>\n",
       "      <th>remorse</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>approval</th>\n",
       "      <th>confusion</th>\n",
       "      <th>...</th>\n",
       "      <th>embarrassment</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>disgust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>excitement</th>\n",
       "      <th>pride</th>\n",
       "      <th>E</th>\n",
       "      <th>O</th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>0.689650</td>\n",
       "      <td>0.188368</td>\n",
       "      <td>0.126145</td>\n",
       "      <td>0.101215</td>\n",
       "      <td>0.092729</td>\n",
       "      <td>0.046889</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>0.026312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>0.021786</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.108906</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.020744</td>\n",
       "      <td>0.043765</td>\n",
       "      <td>0.021139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004704</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.005139</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>0.009194</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>0.013163</td>\n",
       "      <td>0.443897</td>\n",
       "      <td>0.008598</td>\n",
       "      <td>0.075922</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.018230</td>\n",
       "      <td>0.053725</td>\n",
       "      <td>0.321709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.087438</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>0.209207</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.113511</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.058189</td>\n",
       "      <td>0.160561</td>\n",
       "      <td>0.206875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001879</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.149465</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.014468</td>\n",
       "      <td>0.063388</td>\n",
       "      <td>0.058207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename                                              posts   sadness  \\\n",
       "0         0  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...  0.689650   \n",
       "1         1  'I'm finding the lack of me in these posts ver...  0.003080   \n",
       "2         2  'Good one  _____   https://www.youtube.com/wat...  0.013163   \n",
       "3         3  'Dear INTP,   I enjoyed our conversation the o...  0.010823   \n",
       "4         4  'You're fired.|||That's another silly misconce...  0.002441   \n",
       "\n",
       "   curiosity    caring   neutral   remorse  disappointment  approval  \\\n",
       "0   0.188368  0.126145  0.101215  0.092729        0.046889  0.032194   \n",
       "1   0.021786  0.001860  0.108906  0.000811        0.020744  0.043765   \n",
       "2   0.443897  0.008598  0.075922  0.003276        0.018230  0.053725   \n",
       "3   0.209207  0.017589  0.113511  0.005305        0.058189  0.160561   \n",
       "4   0.086538  0.001938  0.149465  0.001085        0.014468  0.063388   \n",
       "\n",
       "   confusion  ...  embarrassment  gratitude   disgust  surprise  excitement  \\\n",
       "0   0.026312  ...       0.002361   0.001960  0.001960  0.001827    0.001702   \n",
       "1   0.021139  ...       0.004704   0.000575  0.005139  0.004308    0.009194   \n",
       "2   0.321709  ...       0.001510   0.087438  0.002965  0.010670    0.005236   \n",
       "3   0.206875  ...       0.001786   0.001879  0.002025  0.002681    0.009717   \n",
       "4   0.058207  ...       0.003577   0.000209  0.006866  0.002247    0.002406   \n",
       "\n",
       "      pride  E  O  A  C  \n",
       "0  0.000448  0  1  1  1  \n",
       "1  0.001004  1  1  0  0  \n",
       "2  0.000360  0  1  0  0  \n",
       "3  0.001332  0  1  0  1  \n",
       "4  0.000445  1  1  0  1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dataset = extract(\"../../corpus/emotion-aware-personality/roberta-base-mbti.pickle\")\n",
    "\n",
    "\n",
    "dataset[\"E\"] = dataset['type'].apply(lambda x: 1 if x[0] == 'E' else 0)\n",
    "dataset[\"O\"] = dataset['type'].apply(lambda x: 1 if x[1] == 'N' else 0)\n",
    "dataset[\"A\"] = dataset['type'].apply(lambda x: 1 if x[2] == 'F' else 0)\n",
    "dataset[\"C\"] = dataset['type'].apply(lambda x: 1 if x[3] == 'J' else 0)\n",
    "\n",
    "dataset = dataset.drop(['type'], axis=1)\n",
    "\n",
    "#save\n",
    "save_dataset(dataset, \"../../corpus/emotion-aware-personality\", \"roberta-base-mbti\")\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phdwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
