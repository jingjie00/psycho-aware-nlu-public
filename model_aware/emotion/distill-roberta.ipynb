{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas pickle file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "def extract(path):\n",
    "    print(os.getcwd())\n",
    "    file = open(path, 'rb')\n",
    "    object_file = pickle.load(file)\n",
    "    file.close()\n",
    "    return object_file\n",
    "\n",
    "def save_dataset(item, dir, name):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    path = dir+\"/\"+name+\".pickle\"\n",
    "    pickle.dump(item, open(path, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingjietan/anaconda3/envs/phdwork/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/Users/jingjietan/anaconda3/envs/phdwork/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 2/2 [00:00<00:00, 14.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      anger   disgust      fear       joy   neutral   sadness  surprise\n",
      "0  0.001381  0.000329  0.000380  0.965741  0.002609  0.013528  0.016032\n",
      "1  0.076094  0.195284  0.016262  0.011164  0.626966  0.050076  0.024154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True, max_length=512)\n",
    "\n",
    "def convert_list_to_dict(list_of_dicts):\n",
    "    result_dict = {}\n",
    "    for item in list_of_dicts:\n",
    "        label = item.get(\"label\")\n",
    "        score = item.get(\"score\")\n",
    "        if label is not None and score is not None:\n",
    "            result_dict[label] = score\n",
    "    return result_dict\n",
    "\n",
    "def convert_emotion(sentences):\n",
    "    # Assuming classifier and convert_list_to_dict are defined elsewhere\n",
    "\n",
    "    # Initialize an empty list to store individual DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Process each sentence in the list\n",
    "    for sentence in tqdm(sentences):\n",
    "        emo = classifier(sentence)  # Assuming classifier returns emotions for a single sentence\n",
    "        out = convert_list_to_dict(emo[0])  # Assuming convert_list_to_dict processes the emotion list\n",
    "        df = pd.DataFrame(out, index=[0])\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "print(convert_emotion([\"I am happy\",\"he\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8675/8675 [29:08<00:00,  4.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mbti = pd.read_csv('../../corpus/mbti.csv')\n",
    "sentence_list = mbti['posts'].tolist()\n",
    "emotion_df_list = convert_emotion(sentence_list)\n",
    "mbti_ready = pd.concat([mbti, emotion_df_list], axis=1)\n",
    "mbti_ready.head()\n",
    "save_dataset(mbti_ready, \"../../corpus/emotion-aware-personality\", \"distill-roberta-mbti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jingjietan/Desktop/PRaware/model_aware/emotion\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>posts</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>neutral</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>E</th>\n",
       "      <th>O</th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.019847</td>\n",
       "      <td>0.056881</td>\n",
       "      <td>0.194236</td>\n",
       "      <td>0.636635</td>\n",
       "      <td>0.082404</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>0.010766</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.904504</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.034155</td>\n",
       "      <td>0.008474</td>\n",
       "      <td>0.030315</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>0.042915</td>\n",
       "      <td>0.024796</td>\n",
       "      <td>0.769279</td>\n",
       "      <td>0.011808</td>\n",
       "      <td>0.051410</td>\n",
       "      <td>0.042462</td>\n",
       "      <td>0.057330</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>0.022988</td>\n",
       "      <td>0.010755</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.212752</td>\n",
       "      <td>0.178759</td>\n",
       "      <td>0.437566</td>\n",
       "      <td>0.117131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>0.370680</td>\n",
       "      <td>0.105621</td>\n",
       "      <td>0.037517</td>\n",
       "      <td>0.012559</td>\n",
       "      <td>0.342921</td>\n",
       "      <td>0.057501</td>\n",
       "      <td>0.073202</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filename                                              posts     anger  \\\n",
       "0         0  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...  0.008167   \n",
       "1         1  'I'm finding the lack of me in these posts ver...  0.010766   \n",
       "2         2  'Good one  _____   https://www.youtube.com/wat...  0.042915   \n",
       "3         3  'Dear INTP,   I enjoyed our conversation the o...  0.022988   \n",
       "4         4  'You're fired.|||That's another silly misconce...  0.370680   \n",
       "\n",
       "    disgust      fear       joy   neutral   sadness  surprise  E  O  A  C  \n",
       "0  0.001831  0.019847  0.056881  0.194236  0.636635  0.082404  0  1  1  1  \n",
       "1  0.006015  0.904504  0.005772  0.034155  0.008474  0.030315  1  1  0  0  \n",
       "2  0.024796  0.769279  0.011808  0.051410  0.042462  0.057330  0  1  0  0  \n",
       "3  0.010755  0.020050  0.212752  0.178759  0.437566  0.117131  0  1  0  1  \n",
       "4  0.105621  0.037517  0.012559  0.342921  0.057501  0.073202  1  1  0  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dataset = extract(\"../../corpus/emotion-aware-personality/distill-roberta-mbti.pickle\")\n",
    "\n",
    "\n",
    "dataset[\"E\"] = dataset['type'].apply(lambda x: 1 if x[0] == 'E' else 0)\n",
    "dataset[\"O\"] = dataset['type'].apply(lambda x: 1 if x[1] == 'N' else 0)\n",
    "dataset[\"A\"] = dataset['type'].apply(lambda x: 1 if x[2] == 'F' else 0)\n",
    "dataset[\"C\"] = dataset['type'].apply(lambda x: 1 if x[3] == 'J' else 0)\n",
    "\n",
    "dataset = dataset.drop(['type'], axis=1)\n",
    "\n",
    "#save\n",
    "save_dataset(dataset, \"../../corpus/emotion-aware-personality\", \"distill-roberta-mbti\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [48:14<00:00, 17.28it/s] \n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv('../../corpus/imdb.csv')\n",
    "imdb['sentiment'] = imdb['sentiment'].replace('positive', 1)\n",
    "imdb['sentiment'] = imdb['sentiment'].replace('negative', 0)\n",
    "sentence_list = imdb['review'].tolist()\n",
    "emotion_df_list = convert_emotion(sentence_list)\n",
    "imdb_ready = pd.concat([imdb, emotion_df_list], axis=1)\n",
    "save_dataset(imdb_ready, \"../../corpus/emotion-aware-sentiment\", \"distill-roberta-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:48<00:00,  6.94it/s]\n"
     ]
    }
   ],
   "source": [
    "moviereview = pd.read_csv('../../corpus/movie-review.csv')\n",
    "sentence_list = moviereview['content'].tolist()\n",
    "emotion_df_list = convert_emotion(sentence_list)\n",
    "moviereview_ready = pd.concat([moviereview, emotion_df_list], axis=1)\n",
    "save_dataset(moviereview_ready, \"../../corpus/emotion-aware-sentiment\", \"distill-roberta-moviereview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1895/1895 [02:40<00:00, 11.80it/s]\n"
     ]
    }
   ],
   "source": [
    "sdcnl = pd.read_csv('../../corpus/sdcnl.csv')\n",
    "\n",
    "# merge title and selftext column together, both have string value\n",
    "sdcnl['text'] = sdcnl['title'].astype(str) + \" | \" + sdcnl['selftext'].astype(str)\n",
    "\n",
    "# drop all column except text and is_suicide column\n",
    "sdcnl = sdcnl[['text', 'is_suicide']]\n",
    "\n",
    "sdcnl.head()\n",
    "\n",
    "sentence_list = sdcnl['text'].tolist()\n",
    "emotion_df_list = convert_emotion(sentence_list)\n",
    "sdcnl_ready = pd.concat([sdcnl, emotion_df_list], axis=1)\n",
    "sdcnl_ready.head()\n",
    "save_dataset(sdcnl_ready, \"../../corpus/emotion-aware-depression\", \"distill-roberta-sdcnl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [11:33<00:00, 28.85it/s]\n"
     ]
    }
   ],
   "source": [
    "twitter = pd.read_csv('../../corpus/mental-health-twitter.csv')\n",
    "twitter = twitter[[\"post_text\",\"label\"]]\n",
    "sentence_list = twitter['post_text'].tolist()\n",
    "emotion_df_list = convert_emotion(sentence_list)\n",
    "twitter_ready = pd.concat([twitter, emotion_df_list], axis=1)\n",
    "twitter_ready.head()\n",
    "save_dataset(twitter_ready, \"../../corpus/emotion-aware-depression\", \"distill-roberta-twitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phdwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
