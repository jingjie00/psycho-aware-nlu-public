{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_O\n",
      "[Epoch  1/200]: train_loss = 0.5179, validation_loss = 0.5253, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch  2/200]: train_loss = 0.4981, validation_loss = 0.5101, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch  3/200]: train_loss = 0.4822, validation_loss = 0.4972, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch  4/200]: train_loss = 0.4694, validation_loss = 0.4885, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch  5/200]: train_loss = 0.4615, validation_loss = 0.4839, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch  6/200]: train_loss = 0.4573, validation_loss = 0.4816, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch  7/200]: train_loss = 0.4550, validation_loss = 0.4803, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch  8/200]: train_loss = 0.4536, validation_loss = 0.4794, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch  9/200]: train_loss = 0.4528, validation_loss = 0.4788, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch 10/200]: train_loss = 0.4522, validation_loss = 0.4782, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch 11/200]: train_loss = 0.4517, validation_loss = 0.4774, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch 12/200]: train_loss = 0.4510, validation_loss = 0.4749, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch 13/200]: train_loss = 0.4496, validation_loss = 0.4661, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch 14/200]: train_loss = 0.4451, validation_loss = 0.4550, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch 15/200]: train_loss = 0.4380, validation_loss = 0.4468, RA = 0.8624, BA: 0.5000, CM:(1197, 191, 0, 0)\n",
      "[Epoch 16/200]: train_loss = 0.4295, validation_loss = 0.4356, RA = 0.8782, BA: 0.5730, CM:(1190, 162, 29, 7)\n",
      "[Epoch 17/200]: train_loss = 0.4199, validation_loss = 0.4242, RA = 0.8898, BA: 0.6303, CM:(1183, 139, 52, 14)\n",
      "[Epoch 18/200]: train_loss = 0.4111, validation_loss = 0.4153, RA = 0.8934, BA: 0.6566, CM:(1177, 128, 63, 20)\n",
      "[Epoch 19/200]: train_loss = 0.4040, validation_loss = 0.4099, RA = 0.8984, BA: 0.6837, CM:(1173, 117, 74, 24)\n",
      "[Epoch 20/200]: train_loss = 0.3987, validation_loss = 0.4065, RA = 0.8970, BA: 0.6895, CM:(1168, 114, 77, 29)\n",
      "[Epoch 21/200]: train_loss = 0.3946, validation_loss = 0.4043, RA = 0.8963, BA: 0.6890, CM:(1167, 114, 77, 30)\n",
      "[Epoch 22/200]: train_loss = 0.3914, validation_loss = 0.4028, RA = 0.8941, BA: 0.6878, CM:(1164, 114, 77, 33)\n",
      "[Epoch 23/200]: train_loss = 0.3887, validation_loss = 0.4017, RA = 0.8919, BA: 0.6909, CM:(1159, 112, 79, 38)\n",
      "[Epoch 24/200]: train_loss = 0.3865, validation_loss = 0.4009, RA = 0.8919, BA: 0.6953, CM:(1157, 110, 81, 40)\n",
      "[Epoch 25/200]: train_loss = 0.3847, validation_loss = 0.4003, RA = 0.8927, BA: 0.7002, CM:(1156, 108, 83, 41)\n",
      "[Epoch 26/200]: train_loss = 0.3831, validation_loss = 0.3999, RA = 0.8927, BA: 0.7002, CM:(1156, 108, 83, 41)\n",
      "[Epoch 27/200]: train_loss = 0.3817, validation_loss = 0.3995, RA = 0.8912, BA: 0.7015, CM:(1153, 107, 84, 44)\n",
      "[Epoch 28/200]: train_loss = 0.3805, validation_loss = 0.3992, RA = 0.8905, BA: 0.7011, CM:(1152, 107, 84, 45)\n",
      "[Epoch 29/200]: train_loss = 0.3794, validation_loss = 0.3990, RA = 0.8890, BA: 0.6981, CM:(1151, 108, 83, 46)\n",
      "[Epoch 30/200]: train_loss = 0.3785, validation_loss = 0.3988, RA = 0.8876, BA: 0.6972, CM:(1149, 108, 83, 48)\n",
      "[Epoch 31/200]: train_loss = 0.3776, validation_loss = 0.3986, RA = 0.8869, BA: 0.6990, CM:(1147, 107, 84, 50)\n",
      "[Epoch 32/200]: train_loss = 0.3769, validation_loss = 0.3985, RA = 0.8876, BA: 0.7016, CM:(1147, 106, 85, 50)\n",
      "[Epoch 33/200]: train_loss = 0.3762, validation_loss = 0.3984, RA = 0.8862, BA: 0.7030, CM:(1144, 105, 86, 53)\n",
      "[Epoch 34/200]: train_loss = 0.3756, validation_loss = 0.3983, RA = 0.8862, BA: 0.7030, CM:(1144, 105, 86, 53)\n",
      "[Epoch 35/200]: train_loss = 0.3751, validation_loss = 0.3982, RA = 0.8862, BA: 0.7008, CM:(1145, 106, 85, 52)\n",
      "[Epoch 36/200]: train_loss = 0.3746, validation_loss = 0.3981, RA = 0.8869, BA: 0.7012, CM:(1146, 106, 85, 51)\n",
      "[Epoch 37/200]: train_loss = 0.3742, validation_loss = 0.3980, RA = 0.8876, BA: 0.6950, CM:(1150, 109, 82, 47)\n",
      "[Epoch 38/200]: train_loss = 0.3738, validation_loss = 0.3980, RA = 0.8898, BA: 0.6963, CM:(1153, 109, 82, 44)\n",
      "[Epoch 39/200]: train_loss = 0.3735, validation_loss = 0.3979, RA = 0.8898, BA: 0.6963, CM:(1153, 109, 82, 44)\n",
      "[Epoch 40/200]: train_loss = 0.3731, validation_loss = 0.3979, RA = 0.8883, BA: 0.6910, CM:(1153, 111, 80, 44)\n",
      "[Epoch 41/200]: train_loss = 0.3729, validation_loss = 0.3979, RA = 0.8883, BA: 0.6910, CM:(1153, 111, 80, 44)\n",
      "[Epoch 42/200]: train_loss = 0.3726, validation_loss = 0.3978, RA = 0.8898, BA: 0.6919, CM:(1155, 111, 80, 42)\n",
      "[Epoch 43/200]: train_loss = 0.3724, validation_loss = 0.3978, RA = 0.8912, BA: 0.6927, CM:(1157, 111, 80, 40)\n",
      "[Epoch 44/200]: train_loss = 0.3721, validation_loss = 0.3978, RA = 0.8905, BA: 0.6945, CM:(1155, 110, 81, 42)\n",
      "[Epoch 45/200]: train_loss = 0.3719, validation_loss = 0.3977, RA = 0.8912, BA: 0.6971, CM:(1155, 109, 82, 42)\n",
      "[Epoch 46/200]: train_loss = 0.3717, validation_loss = 0.3977, RA = 0.8905, BA: 0.6967, CM:(1154, 109, 82, 43)\n",
      "[Epoch 47/200]: train_loss = 0.3716, validation_loss = 0.3977, RA = 0.8890, BA: 0.6959, CM:(1152, 109, 82, 45)\n",
      "[Epoch 48/200]: train_loss = 0.3714, validation_loss = 0.3976, RA = 0.8890, BA: 0.6959, CM:(1152, 109, 82, 45)\n",
      "[Epoch 49/200]: train_loss = 0.3713, validation_loss = 0.3976, RA = 0.8898, BA: 0.6941, CM:(1154, 110, 81, 43)\n",
      "[Epoch 50/200]: train_loss = 0.3711, validation_loss = 0.3976, RA = 0.8876, BA: 0.6928, CM:(1151, 110, 81, 46)\n",
      "[Epoch 51/200]: train_loss = 0.3710, validation_loss = 0.3976, RA = 0.8876, BA: 0.6928, CM:(1151, 110, 81, 46)\n",
      "[Epoch 52/200]: train_loss = 0.3709, validation_loss = 0.3976, RA = 0.8890, BA: 0.6981, CM:(1151, 108, 83, 46)\n",
      "[Epoch 53/200]: train_loss = 0.3708, validation_loss = 0.3976, RA = 0.8883, BA: 0.6954, CM:(1151, 109, 82, 46)\n",
      "[Epoch 54/200]: train_loss = 0.3707, validation_loss = 0.3975, RA = 0.8876, BA: 0.6906, CM:(1152, 111, 80, 45)\n",
      "[Epoch 55/200]: train_loss = 0.3706, validation_loss = 0.3975, RA = 0.8876, BA: 0.6906, CM:(1152, 111, 80, 45)\n",
      "[Epoch 56/200]: train_loss = 0.3706, validation_loss = 0.3975, RA = 0.8869, BA: 0.6880, CM:(1152, 112, 79, 45)\n",
      "[Epoch 57/200]: train_loss = 0.3705, validation_loss = 0.3975, RA = 0.8869, BA: 0.6880, CM:(1152, 112, 79, 45)\n",
      "[Epoch 58/200]: train_loss = 0.3705, validation_loss = 0.3975, RA = 0.8869, BA: 0.6880, CM:(1152, 112, 79, 45)\n",
      "[Epoch 59/200]: train_loss = 0.3704, validation_loss = 0.3975, RA = 0.8876, BA: 0.6884, CM:(1153, 112, 79, 44)\n",
      "[Epoch 60/200]: train_loss = 0.3704, validation_loss = 0.3975, RA = 0.8869, BA: 0.6858, CM:(1153, 113, 78, 44)\n",
      "[Epoch 61/200]: train_loss = 0.3703, validation_loss = 0.3975, RA = 0.8869, BA: 0.6858, CM:(1153, 113, 78, 44)\n",
      "[Epoch 62/200]: train_loss = 0.3703, validation_loss = 0.3975, RA = 0.8862, BA: 0.6854, CM:(1152, 113, 78, 45)\n",
      "[Epoch 63/200]: train_loss = 0.3703, validation_loss = 0.3975, RA = 0.8862, BA: 0.6854, CM:(1152, 113, 78, 45)\n",
      "[Epoch 64/200]: train_loss = 0.3702, validation_loss = 0.3975, RA = 0.8862, BA: 0.6854, CM:(1152, 113, 78, 45)\n",
      "[Epoch 65/200]: train_loss = 0.3702, validation_loss = 0.3975, RA = 0.8854, BA: 0.6850, CM:(1151, 113, 78, 46)\n",
      "[Epoch 66/200]: train_loss = 0.3702, validation_loss = 0.3975, RA = 0.8876, BA: 0.6928, CM:(1151, 110, 81, 46)\n",
      "[Epoch 67/200]: train_loss = 0.3702, validation_loss = 0.3975, RA = 0.8862, BA: 0.6986, CM:(1146, 107, 84, 51)\n",
      "[Epoch 68/200]: train_loss = 0.3701, validation_loss = 0.3974, RA = 0.8826, BA: 0.6987, CM:(1140, 106, 85, 57)\n",
      "[Epoch 69/200]: train_loss = 0.3700, validation_loss = 0.3974, RA = 0.8826, BA: 0.6877, CM:(1145, 111, 80, 52)\n",
      "[Epoch 70/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8833, BA: 0.6881, CM:(1146, 111, 80, 51)\n",
      "[Epoch 71/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8833, BA: 0.6881, CM:(1146, 111, 80, 51)\n",
      "[Epoch 72/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8840, BA: 0.6885, CM:(1147, 111, 80, 50)\n",
      "[Epoch 73/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8826, BA: 0.6855, CM:(1146, 112, 79, 51)\n",
      "[Epoch 74/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8826, BA: 0.6855, CM:(1146, 112, 79, 51)\n",
      "[Epoch 75/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 76/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 77/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 78/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 79/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 80/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8811, BA: 0.6847, CM:(1144, 112, 79, 53)\n",
      "[Epoch 81/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 82/200]: train_loss = 0.3699, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 83/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 84/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 85/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 86/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 87/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 88/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 89/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 90/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 91/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 92/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 93/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 94/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8811, BA: 0.6847, CM:(1144, 112, 79, 53)\n",
      "[Epoch 95/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8811, BA: 0.6847, CM:(1144, 112, 79, 53)\n",
      "[Epoch 96/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8811, BA: 0.6847, CM:(1144, 112, 79, 53)\n",
      "[Epoch 97/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8811, BA: 0.6847, CM:(1144, 112, 79, 53)\n",
      "[Epoch 98/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8811, BA: 0.6847, CM:(1144, 112, 79, 53)\n",
      "[Epoch 99/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8804, BA: 0.6842, CM:(1143, 112, 79, 54)\n",
      "[Epoch 100/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 101/200]: train_loss = 0.3698, validation_loss = 0.3974, RA = 0.8818, BA: 0.6917, CM:(1142, 109, 82, 55)\n",
      "[Epoch 102/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 103/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8826, BA: 0.6877, CM:(1145, 111, 80, 52)\n",
      "[Epoch 104/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8826, BA: 0.6877, CM:(1145, 111, 80, 52)\n",
      "[Epoch 105/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6851, CM:(1145, 112, 79, 52)\n",
      "[Epoch 106/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8804, BA: 0.6842, CM:(1143, 112, 79, 54)\n",
      "[Epoch 107/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8804, BA: 0.6842, CM:(1143, 112, 79, 54)\n",
      "[Epoch 108/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 109/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 110/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 111/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 112/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 113/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 114/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 115/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 116/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 117/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 118/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 119/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6869, CM:(1143, 111, 80, 54)\n",
      "[Epoch 120/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 121/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 122/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 123/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 124/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 125/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 126/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 127/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 128/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 129/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 130/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 131/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 132/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 133/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 134/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 135/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 136/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 137/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 138/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 139/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 140/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 141/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 142/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 143/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 144/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 145/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 146/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 147/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 148/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 149/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 150/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 151/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 152/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 153/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 154/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 155/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 156/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 157/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 158/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 159/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 160/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 161/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 162/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 163/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6891, CM:(1142, 110, 81, 55)\n",
      "[Epoch 164/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8818, BA: 0.6917, CM:(1142, 109, 82, 55)\n",
      "[Epoch 165/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8811, BA: 0.6913, CM:(1141, 109, 82, 56)\n",
      "[Epoch 166/200]: train_loss = 0.3697, validation_loss = 0.3974, RA = 0.8804, BA: 0.7107, CM:(1131, 100, 91, 66)\n",
      "[Epoch 167/200]: train_loss = 0.3696, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 168/200]: train_loss = 0.3696, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 169/200]: train_loss = 0.3696, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 170/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 171/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 172/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 173/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 174/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 175/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 176/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 177/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 178/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 179/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 180/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 181/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 182/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 183/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 184/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 185/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 186/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 187/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 188/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 189/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 190/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 191/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 192/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 193/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 194/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6921, CM:(1143, 109, 82, 54)\n",
      "[Epoch 195/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8818, BA: 0.6895, CM:(1143, 110, 81, 54)\n",
      "[Epoch 196/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8833, BA: 0.6881, CM:(1146, 111, 80, 51)\n",
      "[Epoch 197/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8898, BA: 0.6501, CM:(1174, 130, 61, 23)\n",
      "[Epoch 198/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8797, BA: 0.6948, CM:(1137, 107, 84, 60)\n",
      "[Epoch 199/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6943, CM:(1142, 108, 83, 55)\n",
      "[Epoch 200/200]: train_loss = 0.3695, validation_loss = 0.3974, RA = 0.8826, BA: 0.6943, CM:(1142, 108, 83, 55)\n",
      "(1431, 156, 84, 64)\n",
      "P_C\n",
      "[Epoch  1/200]: train_loss = 0.7629, validation_loss = 0.7485, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch  2/200]: train_loss = 0.7523, validation_loss = 0.7396, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch  3/200]: train_loss = 0.7403, validation_loss = 0.7267, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch  4/200]: train_loss = 0.7250, validation_loss = 0.7130, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch  5/200]: train_loss = 0.7117, validation_loss = 0.7036, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch  6/200]: train_loss = 0.7036, validation_loss = 0.6987, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch  7/200]: train_loss = 0.6994, validation_loss = 0.6963, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch  8/200]: train_loss = 0.6972, validation_loss = 0.6951, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch  9/200]: train_loss = 0.6959, validation_loss = 0.6944, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 10/200]: train_loss = 0.6952, validation_loss = 0.6940, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 11/200]: train_loss = 0.6947, validation_loss = 0.6937, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 12/200]: train_loss = 0.6943, validation_loss = 0.6935, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 13/200]: train_loss = 0.6941, validation_loss = 0.6933, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 14/200]: train_loss = 0.6938, validation_loss = 0.6930, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 15/200]: train_loss = 0.6936, validation_loss = 0.6926, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 16/200]: train_loss = 0.6933, validation_loss = 0.6919, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 17/200]: train_loss = 0.6926, validation_loss = 0.6877, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 18/200]: train_loss = 0.6864, validation_loss = 0.6653, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 19/200]: train_loss = 0.6754, validation_loss = 0.6516, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 20/200]: train_loss = 0.6675, validation_loss = 0.6402, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 21/200]: train_loss = 0.6614, validation_loss = 0.6332, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 22/200]: train_loss = 0.6564, validation_loss = 0.6289, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 23/200]: train_loss = 0.6519, validation_loss = 0.6256, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 24/200]: train_loss = 0.6478, validation_loss = 0.6228, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 25/200]: train_loss = 0.6440, validation_loss = 0.6203, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 26/200]: train_loss = 0.6403, validation_loss = 0.6179, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 27/200]: train_loss = 0.6368, validation_loss = 0.6155, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 28/200]: train_loss = 0.6336, validation_loss = 0.6126, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 29/200]: train_loss = 0.6305, validation_loss = 0.6094, RA = 0.6045, BA: 0.5000, CM:(0, 0, 839, 549)\n",
      "[Epoch 30/200]: train_loss = 0.6276, validation_loss = 0.6071, RA = 0.7118, BA: 0.6439, CM:(175, 26, 813, 374)\n",
      "[Epoch 31/200]: train_loss = 0.6247, validation_loss = 0.6050, RA = 0.7219, BA: 0.6604, CM:(201, 38, 801, 348)\n",
      "[Epoch 32/200]: train_loss = 0.6220, validation_loss = 0.6037, RA = 0.7363, BA: 0.6834, CM:(236, 53, 786, 313)\n",
      "[Epoch 33/200]: train_loss = 0.6192, validation_loss = 0.6019, RA = 0.7442, BA: 0.6965, CM:(257, 63, 776, 292)\n",
      "[Epoch 34/200]: train_loss = 0.6165, validation_loss = 0.5991, RA = 0.7414, BA: 0.6922, CM:(251, 61, 778, 298)\n",
      "[Epoch 35/200]: train_loss = 0.6140, validation_loss = 0.5976, RA = 0.7349, BA: 0.6800, CM:(229, 48, 791, 320)\n",
      "[Epoch 36/200]: train_loss = 0.6114, validation_loss = 0.5946, RA = 0.7284, BA: 0.6714, CM:(219, 47, 792, 330)\n",
      "[Epoch 37/200]: train_loss = 0.6090, validation_loss = 0.5931, RA = 0.7370, BA: 0.6868, CM:(245, 61, 778, 304)\n",
      "[Epoch 38/200]: train_loss = 0.6068, validation_loss = 0.5918, RA = 0.7464, BA: 0.7037, CM:(274, 77, 762, 275)\n",
      "[Epoch 39/200]: train_loss = 0.6046, validation_loss = 0.5900, RA = 0.7478, BA: 0.7058, CM:(277, 78, 761, 272)\n",
      "[Epoch 40/200]: train_loss = 0.6027, validation_loss = 0.5883, RA = 0.7435, BA: 0.6997, CM:(269, 76, 763, 280)\n",
      "[Epoch 41/200]: train_loss = 0.6009, validation_loss = 0.5865, RA = 0.7378, BA: 0.6883, CM:(248, 63, 776, 301)\n",
      "[Epoch 42/200]: train_loss = 0.5990, validation_loss = 0.5846, RA = 0.7378, BA: 0.6921, CM:(260, 75, 764, 289)\n",
      "[Epoch 43/200]: train_loss = 0.5971, validation_loss = 0.5834, RA = 0.7450, BA: 0.7025, CM:(274, 79, 760, 275)\n",
      "[Epoch 44/200]: train_loss = 0.5956, validation_loss = 0.5825, RA = 0.7414, BA: 0.7045, CM:(290, 100, 739, 259)\n",
      "[Epoch 45/200]: train_loss = 0.5939, validation_loss = 0.5808, RA = 0.7428, BA: 0.7051, CM:(288, 96, 743, 261)\n",
      "[Epoch 46/200]: train_loss = 0.5924, validation_loss = 0.5796, RA = 0.7457, BA: 0.7040, CM:(277, 81, 758, 272)\n",
      "[Epoch 47/200]: train_loss = 0.5909, validation_loss = 0.5781, RA = 0.7414, BA: 0.6976, CM:(268, 78, 761, 281)\n",
      "[Epoch 48/200]: train_loss = 0.5896, validation_loss = 0.5769, RA = 0.7406, BA: 0.6995, CM:(276, 87, 752, 273)\n",
      "[Epoch 49/200]: train_loss = 0.5882, validation_loss = 0.5761, RA = 0.7421, BA: 0.7057, CM:(292, 101, 738, 257)\n",
      "[Epoch 50/200]: train_loss = 0.5869, validation_loss = 0.5752, RA = 0.7478, BA: 0.7146, CM:(305, 106, 733, 244)\n",
      "[Epoch 51/200]: train_loss = 0.5858, validation_loss = 0.5739, RA = 0.7464, BA: 0.7122, CM:(301, 104, 735, 248)\n",
      "[Epoch 52/200]: train_loss = 0.5847, validation_loss = 0.5731, RA = 0.7428, BA: 0.7045, CM:(286, 94, 745, 263)\n",
      "[Epoch 53/200]: train_loss = 0.5838, validation_loss = 0.5716, RA = 0.7421, BA: 0.7017, CM:(279, 88, 751, 270)\n",
      "[Epoch 54/200]: train_loss = 0.5827, validation_loss = 0.5709, RA = 0.7450, BA: 0.7091, CM:(295, 100, 739, 254)\n",
      "[Epoch 55/200]: train_loss = 0.5815, validation_loss = 0.5702, RA = 0.7442, BA: 0.7116, CM:(305, 111, 728, 244)\n",
      "[Epoch 56/200]: train_loss = 0.5806, validation_loss = 0.5692, RA = 0.7442, BA: 0.7116, CM:(305, 111, 728, 244)\n",
      "[Epoch 57/200]: train_loss = 0.5798, validation_loss = 0.5682, RA = 0.7428, BA: 0.7060, CM:(291, 99, 740, 258)\n",
      "[Epoch 58/200]: train_loss = 0.5790, validation_loss = 0.5678, RA = 0.7421, BA: 0.7020, CM:(280, 89, 750, 269)\n",
      "[Epoch 59/200]: train_loss = 0.5782, validation_loss = 0.5667, RA = 0.7414, BA: 0.7020, CM:(282, 92, 747, 267)\n",
      "[Epoch 60/200]: train_loss = 0.5774, validation_loss = 0.5660, RA = 0.7435, BA: 0.7069, CM:(292, 99, 740, 257)\n",
      "[Epoch 61/200]: train_loss = 0.5766, validation_loss = 0.5656, RA = 0.7428, BA: 0.7038, CM:(284, 92, 747, 265)\n",
      "[Epoch 62/200]: train_loss = 0.5760, validation_loss = 0.5669, RA = 0.7428, BA: 0.6972, CM:(263, 71, 768, 286)\n",
      "[Epoch 63/200]: train_loss = 0.5758, validation_loss = 0.5639, RA = 0.7341, BA: 0.6857, CM:(249, 69, 770, 300)\n",
      "[Epoch 64/200]: train_loss = 0.5753, validation_loss = 0.5633, RA = 0.7378, BA: 0.6918, CM:(259, 74, 765, 290)\n",
      "[Epoch 65/200]: train_loss = 0.5746, validation_loss = 0.5628, RA = 0.7363, BA: 0.6890, CM:(254, 71, 768, 295)\n",
      "[Epoch 66/200]: train_loss = 0.5735, validation_loss = 0.5627, RA = 0.7269, BA: 0.6753, CM:(235, 65, 774, 314)\n",
      "[Epoch 67/200]: train_loss = 0.5729, validation_loss = 0.5616, RA = 0.7320, BA: 0.6823, CM:(244, 67, 772, 305)\n",
      "[Epoch 68/200]: train_loss = 0.5727, validation_loss = 0.5611, RA = 0.7298, BA: 0.6792, CM:(240, 66, 773, 309)\n",
      "[Epoch 69/200]: train_loss = 0.5723, validation_loss = 0.5607, RA = 0.7291, BA: 0.6787, CM:(240, 67, 772, 309)\n",
      "[Epoch 70/200]: train_loss = 0.5721, validation_loss = 0.5602, RA = 0.7298, BA: 0.6786, CM:(238, 64, 775, 311)\n",
      "[Epoch 71/200]: train_loss = 0.5714, validation_loss = 0.5597, RA = 0.7313, BA: 0.6823, CM:(246, 70, 769, 303)\n",
      "[Epoch 72/200]: train_loss = 0.5706, validation_loss = 0.5593, RA = 0.7392, BA: 0.6955, CM:(267, 80, 759, 282)\n",
      "[Epoch 73/200]: train_loss = 0.5698, validation_loss = 0.5590, RA = 0.7392, BA: 0.7002, CM:(282, 95, 744, 267)\n",
      "[Epoch 74/200]: train_loss = 0.5697, validation_loss = 0.5586, RA = 0.7406, BA: 0.7024, CM:(285, 96, 743, 264)\n",
      "[Epoch 75/200]: train_loss = 0.5693, validation_loss = 0.5583, RA = 0.7414, BA: 0.7058, CM:(294, 104, 735, 255)\n",
      "[Epoch 76/200]: train_loss = 0.5688, validation_loss = 0.5581, RA = 0.7363, BA: 0.7054, CM:(306, 123, 716, 243)\n",
      "[Epoch 77/200]: train_loss = 0.5680, validation_loss = 0.5580, RA = 0.7442, BA: 0.7201, CM:(332, 138, 701, 217)\n",
      "[Epoch 78/200]: train_loss = 0.5673, validation_loss = 0.5575, RA = 0.7406, BA: 0.7184, CM:(336, 147, 692, 213)\n",
      "[Epoch 79/200]: train_loss = 0.5667, validation_loss = 0.5568, RA = 0.7406, BA: 0.7153, CM:(326, 137, 702, 223)\n",
      "[Epoch 80/200]: train_loss = 0.5662, validation_loss = 0.5562, RA = 0.7356, BA: 0.7016, CM:(296, 114, 725, 253)\n",
      "[Epoch 81/200]: train_loss = 0.5656, validation_loss = 0.5558, RA = 0.7399, BA: 0.7021, CM:(286, 98, 741, 263)\n",
      "[Epoch 82/200]: train_loss = 0.5651, validation_loss = 0.5555, RA = 0.7378, BA: 0.6993, CM:(283, 98, 741, 266)\n",
      "[Epoch 83/200]: train_loss = 0.5648, validation_loss = 0.5552, RA = 0.7385, BA: 0.6996, CM:(282, 96, 743, 267)\n",
      "[Epoch 84/200]: train_loss = 0.5644, validation_loss = 0.5549, RA = 0.7378, BA: 0.6987, CM:(281, 96, 743, 268)\n",
      "[Epoch 85/200]: train_loss = 0.5641, validation_loss = 0.5546, RA = 0.7378, BA: 0.6990, CM:(282, 97, 742, 267)\n",
      "[Epoch 86/200]: train_loss = 0.5638, validation_loss = 0.5543, RA = 0.7378, BA: 0.6997, CM:(284, 99, 740, 265)\n",
      "[Epoch 87/200]: train_loss = 0.5635, validation_loss = 0.5540, RA = 0.7378, BA: 0.7003, CM:(286, 101, 738, 263)\n",
      "[Epoch 88/200]: train_loss = 0.5632, validation_loss = 0.5537, RA = 0.7385, BA: 0.7009, CM:(286, 100, 739, 263)\n",
      "[Epoch 89/200]: train_loss = 0.5630, validation_loss = 0.5534, RA = 0.7356, BA: 0.6972, CM:(282, 100, 739, 267)\n",
      "[Epoch 90/200]: train_loss = 0.5627, validation_loss = 0.5532, RA = 0.7341, BA: 0.6938, CM:(275, 95, 744, 274)\n",
      "[Epoch 91/200]: train_loss = 0.5625, validation_loss = 0.5529, RA = 0.7363, BA: 0.6966, CM:(278, 95, 744, 271)\n",
      "[Epoch 92/200]: train_loss = 0.5622, validation_loss = 0.5527, RA = 0.7356, BA: 0.6963, CM:(279, 97, 742, 270)\n",
      "[Epoch 93/200]: train_loss = 0.5620, validation_loss = 0.5525, RA = 0.7356, BA: 0.6979, CM:(284, 102, 737, 265)\n",
      "[Epoch 94/200]: train_loss = 0.5618, validation_loss = 0.5522, RA = 0.7370, BA: 0.7003, CM:(288, 104, 735, 261)\n",
      "[Epoch 95/200]: train_loss = 0.5616, validation_loss = 0.5520, RA = 0.7370, BA: 0.7006, CM:(289, 105, 734, 260)\n",
      "[Epoch 96/200]: train_loss = 0.5613, validation_loss = 0.5518, RA = 0.7363, BA: 0.6997, CM:(288, 105, 734, 261)\n",
      "[Epoch 97/200]: train_loss = 0.5611, validation_loss = 0.5516, RA = 0.7356, BA: 0.6979, CM:(284, 102, 737, 265)\n",
      "[Epoch 98/200]: train_loss = 0.5609, validation_loss = 0.5514, RA = 0.7378, BA: 0.6987, CM:(281, 96, 743, 268)\n",
      "[Epoch 99/200]: train_loss = 0.5607, validation_loss = 0.5512, RA = 0.7334, BA: 0.6929, CM:(274, 95, 744, 275)\n",
      "[Epoch 100/200]: train_loss = 0.5606, validation_loss = 0.5510, RA = 0.7327, BA: 0.6923, CM:(274, 96, 743, 275)\n",
      "[Epoch 101/200]: train_loss = 0.5604, validation_loss = 0.5508, RA = 0.7370, BA: 0.6978, CM:(280, 96, 743, 269)\n",
      "[Epoch 102/200]: train_loss = 0.5602, validation_loss = 0.5506, RA = 0.7363, BA: 0.6978, CM:(282, 99, 740, 267)\n",
      "[Epoch 103/200]: train_loss = 0.5601, validation_loss = 0.5505, RA = 0.7349, BA: 0.6973, CM:(284, 103, 736, 265)\n",
      "[Epoch 104/200]: train_loss = 0.5599, validation_loss = 0.5503, RA = 0.7356, BA: 0.6976, CM:(283, 101, 738, 266)\n",
      "[Epoch 105/200]: train_loss = 0.5597, validation_loss = 0.5501, RA = 0.7349, BA: 0.6976, CM:(285, 104, 735, 264)\n",
      "[Epoch 106/200]: train_loss = 0.5596, validation_loss = 0.5500, RA = 0.7363, BA: 0.6997, CM:(288, 105, 734, 261)\n",
      "[Epoch 107/200]: train_loss = 0.5594, validation_loss = 0.5498, RA = 0.7385, BA: 0.7025, CM:(291, 105, 734, 258)\n",
      "[Epoch 108/200]: train_loss = 0.5593, validation_loss = 0.5496, RA = 0.7349, BA: 0.6973, CM:(284, 103, 736, 265)\n",
      "[Epoch 109/200]: train_loss = 0.5591, validation_loss = 0.5495, RA = 0.7363, BA: 0.6981, CM:(283, 100, 739, 266)\n",
      "[Epoch 110/200]: train_loss = 0.5590, validation_loss = 0.5493, RA = 0.7349, BA: 0.6951, CM:(277, 96, 743, 272)\n",
      "[Epoch 111/200]: train_loss = 0.5588, validation_loss = 0.5492, RA = 0.7334, BA: 0.6932, CM:(275, 96, 743, 274)\n",
      "[Epoch 112/200]: train_loss = 0.5587, validation_loss = 0.5491, RA = 0.7334, BA: 0.6932, CM:(275, 96, 743, 274)\n",
      "[Epoch 113/200]: train_loss = 0.5586, validation_loss = 0.5489, RA = 0.7349, BA: 0.6966, CM:(282, 101, 738, 267)\n",
      "[Epoch 114/200]: train_loss = 0.5584, validation_loss = 0.5488, RA = 0.7349, BA: 0.6966, CM:(282, 101, 738, 267)\n",
      "[Epoch 115/200]: train_loss = 0.5583, validation_loss = 0.5487, RA = 0.7363, BA: 0.6981, CM:(283, 100, 739, 266)\n",
      "[Epoch 116/200]: train_loss = 0.5582, validation_loss = 0.5485, RA = 0.7349, BA: 0.6973, CM:(284, 103, 736, 265)\n",
      "[Epoch 117/200]: train_loss = 0.5580, validation_loss = 0.5484, RA = 0.7363, BA: 0.6994, CM:(287, 104, 735, 262)\n",
      "[Epoch 118/200]: train_loss = 0.5579, validation_loss = 0.5483, RA = 0.7363, BA: 0.6997, CM:(288, 105, 734, 261)\n",
      "[Epoch 119/200]: train_loss = 0.5578, validation_loss = 0.5482, RA = 0.7370, BA: 0.7003, CM:(288, 104, 735, 261)\n",
      "[Epoch 120/200]: train_loss = 0.5577, validation_loss = 0.5481, RA = 0.7356, BA: 0.6985, CM:(286, 104, 735, 263)\n",
      "[Epoch 121/200]: train_loss = 0.5576, validation_loss = 0.5480, RA = 0.7356, BA: 0.6976, CM:(283, 101, 738, 266)\n",
      "[Epoch 122/200]: train_loss = 0.5575, validation_loss = 0.5479, RA = 0.7341, BA: 0.6945, CM:(277, 97, 742, 272)\n",
      "[Epoch 123/200]: train_loss = 0.5574, validation_loss = 0.5477, RA = 0.7356, BA: 0.6957, CM:(277, 95, 744, 272)\n",
      "[Epoch 124/200]: train_loss = 0.5572, validation_loss = 0.5476, RA = 0.7327, BA: 0.6933, CM:(277, 99, 740, 272)\n",
      "[Epoch 125/200]: train_loss = 0.5571, validation_loss = 0.5475, RA = 0.7334, BA: 0.6939, CM:(277, 98, 741, 272)\n",
      "[Epoch 126/200]: train_loss = 0.5570, validation_loss = 0.5474, RA = 0.7327, BA: 0.6945, CM:(281, 103, 736, 268)\n",
      "[Epoch 127/200]: train_loss = 0.5569, validation_loss = 0.5473, RA = 0.7356, BA: 0.6985, CM:(286, 104, 735, 263)\n",
      "[Epoch 128/200]: train_loss = 0.5569, validation_loss = 0.5473, RA = 0.7370, BA: 0.7013, CM:(291, 107, 732, 258)\n",
      "[Epoch 129/200]: train_loss = 0.5567, validation_loss = 0.5472, RA = 0.7334, BA: 0.6967, CM:(286, 107, 732, 263)\n",
      "[Epoch 130/200]: train_loss = 0.5566, validation_loss = 0.5471, RA = 0.7349, BA: 0.6992, CM:(290, 109, 730, 259)\n",
      "[Epoch 131/200]: train_loss = 0.5565, validation_loss = 0.5470, RA = 0.7363, BA: 0.7013, CM:(293, 110, 729, 256)\n",
      "[Epoch 132/200]: train_loss = 0.5565, validation_loss = 0.5469, RA = 0.7363, BA: 0.7010, CM:(292, 109, 730, 257)\n",
      "[Epoch 133/200]: train_loss = 0.5564, validation_loss = 0.5468, RA = 0.7356, BA: 0.6998, CM:(290, 108, 731, 259)\n",
      "[Epoch 134/200]: train_loss = 0.5563, validation_loss = 0.5467, RA = 0.7341, BA: 0.6967, CM:(284, 104, 735, 265)\n",
      "[Epoch 135/200]: train_loss = 0.5562, validation_loss = 0.5467, RA = 0.7334, BA: 0.6945, CM:(279, 100, 739, 270)\n",
      "[Epoch 136/200]: train_loss = 0.5561, validation_loss = 0.5466, RA = 0.7349, BA: 0.6948, CM:(276, 95, 744, 273)\n",
      "[Epoch 137/200]: train_loss = 0.5561, validation_loss = 0.5465, RA = 0.7349, BA: 0.6960, CM:(280, 99, 740, 269)\n",
      "[Epoch 138/200]: train_loss = 0.5560, validation_loss = 0.5464, RA = 0.7341, BA: 0.6964, CM:(283, 103, 736, 266)\n",
      "[Epoch 139/200]: train_loss = 0.5559, validation_loss = 0.5463, RA = 0.7341, BA: 0.6964, CM:(283, 103, 736, 266)\n",
      "[Epoch 140/200]: train_loss = 0.5558, validation_loss = 0.5463, RA = 0.7349, BA: 0.6973, CM:(284, 103, 736, 265)\n",
      "[Epoch 141/200]: train_loss = 0.5558, validation_loss = 0.5462, RA = 0.7320, BA: 0.6955, CM:(286, 109, 730, 263)\n",
      "[Epoch 142/200]: train_loss = 0.5557, validation_loss = 0.5461, RA = 0.7320, BA: 0.6949, CM:(284, 107, 732, 265)\n",
      "[Epoch 143/200]: train_loss = 0.5556, validation_loss = 0.5461, RA = 0.7313, BA: 0.6946, CM:(285, 109, 730, 264)\n",
      "[Epoch 144/200]: train_loss = 0.5556, validation_loss = 0.5460, RA = 0.7334, BA: 0.6954, CM:(282, 103, 736, 267)\n",
      "[Epoch 145/200]: train_loss = 0.5554, validation_loss = 0.5459, RA = 0.7327, BA: 0.6967, CM:(288, 110, 729, 261)\n",
      "[Epoch 146/200]: train_loss = 0.5553, validation_loss = 0.5459, RA = 0.7320, BA: 0.6946, CM:(283, 106, 733, 266)\n",
      "[Epoch 147/200]: train_loss = 0.5553, validation_loss = 0.5458, RA = 0.7334, BA: 0.6951, CM:(281, 102, 737, 268)\n",
      "[Epoch 148/200]: train_loss = 0.5552, validation_loss = 0.5458, RA = 0.7327, BA: 0.6949, CM:(282, 104, 735, 267)\n",
      "[Epoch 149/200]: train_loss = 0.5552, validation_loss = 0.5457, RA = 0.7320, BA: 0.6939, CM:(281, 104, 735, 268)\n",
      "[Epoch 150/200]: train_loss = 0.5551, validation_loss = 0.5456, RA = 0.7341, BA: 0.6957, CM:(281, 101, 738, 268)\n",
      "[Epoch 151/200]: train_loss = 0.5550, validation_loss = 0.5456, RA = 0.7341, BA: 0.6954, CM:(280, 100, 739, 269)\n",
      "[Epoch 152/200]: train_loss = 0.5550, validation_loss = 0.5455, RA = 0.7313, BA: 0.6943, CM:(284, 108, 731, 265)\n",
      "[Epoch 153/200]: train_loss = 0.5549, validation_loss = 0.5455, RA = 0.7341, BA: 0.6957, CM:(281, 101, 738, 268)\n",
      "[Epoch 154/200]: train_loss = 0.5548, validation_loss = 0.5454, RA = 0.7313, BA: 0.6933, CM:(281, 105, 734, 268)\n",
      "[Epoch 155/200]: train_loss = 0.5548, validation_loss = 0.5454, RA = 0.7320, BA: 0.6939, CM:(281, 104, 735, 268)\n",
      "[Epoch 156/200]: train_loss = 0.5547, validation_loss = 0.5453, RA = 0.7320, BA: 0.6939, CM:(281, 104, 735, 268)\n",
      "[Epoch 157/200]: train_loss = 0.5547, validation_loss = 0.5453, RA = 0.7320, BA: 0.6952, CM:(285, 108, 731, 264)\n",
      "[Epoch 158/200]: train_loss = 0.5546, validation_loss = 0.5452, RA = 0.7313, BA: 0.6955, CM:(288, 112, 727, 261)\n",
      "[Epoch 159/200]: train_loss = 0.5545, validation_loss = 0.5452, RA = 0.7313, BA: 0.6974, CM:(294, 118, 721, 255)\n",
      "[Epoch 160/200]: train_loss = 0.5545, validation_loss = 0.5451, RA = 0.7298, BA: 0.6972, CM:(297, 123, 716, 252)\n",
      "[Epoch 161/200]: train_loss = 0.5544, validation_loss = 0.5451, RA = 0.7291, BA: 0.6956, CM:(294, 121, 718, 255)\n",
      "[Epoch 162/200]: train_loss = 0.5544, validation_loss = 0.5450, RA = 0.7320, BA: 0.6968, CM:(290, 113, 726, 259)\n",
      "[Epoch 163/200]: train_loss = 0.5544, validation_loss = 0.5450, RA = 0.7313, BA: 0.6943, CM:(284, 108, 731, 265)\n",
      "[Epoch 164/200]: train_loss = 0.5543, validation_loss = 0.5450, RA = 0.7313, BA: 0.6933, CM:(281, 105, 734, 268)\n",
      "[Epoch 165/200]: train_loss = 0.5543, validation_loss = 0.5449, RA = 0.7313, BA: 0.6927, CM:(279, 103, 736, 270)\n",
      "[Epoch 166/200]: train_loss = 0.5542, validation_loss = 0.5449, RA = 0.7320, BA: 0.6943, CM:(282, 105, 734, 267)\n",
      "[Epoch 167/200]: train_loss = 0.5541, validation_loss = 0.5448, RA = 0.7327, BA: 0.6942, CM:(280, 102, 737, 269)\n",
      "[Epoch 168/200]: train_loss = 0.5541, validation_loss = 0.5448, RA = 0.7327, BA: 0.6942, CM:(280, 102, 737, 269)\n",
      "[Epoch 169/200]: train_loss = 0.5541, validation_loss = 0.5448, RA = 0.7305, BA: 0.6927, CM:(281, 106, 733, 268)\n",
      "[Epoch 170/200]: train_loss = 0.5540, validation_loss = 0.5447, RA = 0.7327, BA: 0.6958, CM:(285, 107, 732, 264)\n",
      "[Epoch 171/200]: train_loss = 0.5540, validation_loss = 0.5447, RA = 0.7284, BA: 0.6941, CM:(291, 119, 720, 258)\n",
      "[Epoch 172/200]: train_loss = 0.5539, validation_loss = 0.5446, RA = 0.7320, BA: 0.6955, CM:(286, 109, 730, 263)\n",
      "[Epoch 173/200]: train_loss = 0.5539, validation_loss = 0.5446, RA = 0.7313, BA: 0.6955, CM:(288, 112, 727, 261)\n",
      "[Epoch 174/200]: train_loss = 0.5539, validation_loss = 0.5446, RA = 0.7291, BA: 0.6947, CM:(291, 118, 721, 258)\n",
      "[Epoch 175/200]: train_loss = 0.5538, validation_loss = 0.5445, RA = 0.7291, BA: 0.6941, CM:(289, 116, 723, 260)\n",
      "[Epoch 176/200]: train_loss = 0.5538, validation_loss = 0.5445, RA = 0.7284, BA: 0.6932, CM:(288, 116, 723, 261)\n",
      "[Epoch 177/200]: train_loss = 0.5538, validation_loss = 0.5445, RA = 0.7298, BA: 0.6940, CM:(287, 113, 726, 262)\n",
      "[Epoch 178/200]: train_loss = 0.5538, validation_loss = 0.5444, RA = 0.7327, BA: 0.6952, CM:(283, 105, 734, 266)\n",
      "[Epoch 179/200]: train_loss = 0.5537, validation_loss = 0.5444, RA = 0.7305, BA: 0.6924, CM:(280, 105, 734, 269)\n",
      "[Epoch 180/200]: train_loss = 0.5537, validation_loss = 0.5444, RA = 0.7320, BA: 0.6936, CM:(280, 103, 736, 269)\n",
      "[Epoch 181/200]: train_loss = 0.5537, validation_loss = 0.5443, RA = 0.7313, BA: 0.6933, CM:(281, 105, 734, 268)\n",
      "[Epoch 182/200]: train_loss = 0.5536, validation_loss = 0.5443, RA = 0.7298, BA: 0.6918, CM:(280, 106, 733, 269)\n",
      "[Epoch 183/200]: train_loss = 0.5536, validation_loss = 0.5443, RA = 0.7320, BA: 0.6949, CM:(284, 107, 732, 265)\n",
      "[Epoch 184/200]: train_loss = 0.5536, validation_loss = 0.5443, RA = 0.7327, BA: 0.6961, CM:(286, 108, 731, 263)\n",
      "[Epoch 185/200]: train_loss = 0.5536, validation_loss = 0.5442, RA = 0.7291, BA: 0.6941, CM:(289, 116, 723, 260)\n",
      "[Epoch 186/200]: train_loss = 0.5535, validation_loss = 0.5442, RA = 0.7291, BA: 0.6938, CM:(288, 115, 724, 261)\n",
      "[Epoch 187/200]: train_loss = 0.5535, validation_loss = 0.5442, RA = 0.7305, BA: 0.6943, CM:(286, 111, 728, 263)\n",
      "[Epoch 188/200]: train_loss = 0.5535, validation_loss = 0.5441, RA = 0.7305, BA: 0.6946, CM:(287, 112, 727, 262)\n",
      "[Epoch 189/200]: train_loss = 0.5534, validation_loss = 0.5441, RA = 0.7291, BA: 0.6947, CM:(291, 118, 721, 258)\n",
      "[Epoch 190/200]: train_loss = 0.5534, validation_loss = 0.5441, RA = 0.7305, BA: 0.6950, CM:(288, 113, 726, 261)\n",
      "[Epoch 191/200]: train_loss = 0.5534, validation_loss = 0.5441, RA = 0.7327, BA: 0.6958, CM:(285, 107, 732, 264)\n",
      "[Epoch 192/200]: train_loss = 0.5534, validation_loss = 0.5441, RA = 0.7327, BA: 0.6949, CM:(282, 104, 735, 267)\n",
      "[Epoch 193/200]: train_loss = 0.5534, validation_loss = 0.5440, RA = 0.7320, BA: 0.6943, CM:(282, 105, 734, 267)\n",
      "[Epoch 194/200]: train_loss = 0.5533, validation_loss = 0.5440, RA = 0.7334, BA: 0.6961, CM:(284, 105, 734, 265)\n",
      "[Epoch 195/200]: train_loss = 0.5533, validation_loss = 0.5440, RA = 0.7327, BA: 0.6955, CM:(284, 106, 733, 265)\n",
      "[Epoch 196/200]: train_loss = 0.5533, validation_loss = 0.5440, RA = 0.7320, BA: 0.6952, CM:(285, 108, 731, 264)\n",
      "[Epoch 197/200]: train_loss = 0.5533, validation_loss = 0.5439, RA = 0.7305, BA: 0.6943, CM:(286, 111, 728, 263)\n",
      "[Epoch 198/200]: train_loss = 0.5532, validation_loss = 0.5439, RA = 0.7320, BA: 0.6955, CM:(286, 109, 730, 263)\n",
      "[Epoch 199/200]: train_loss = 0.5532, validation_loss = 0.5439, RA = 0.7320, BA: 0.6952, CM:(285, 108, 731, 264)\n",
      "[Epoch 200/200]: train_loss = 0.5532, validation_loss = 0.5439, RA = 0.7277, BA: 0.6919, CM:(286, 115, 724, 263)\n",
      "(356, 194, 854, 331)\n",
      "P_E\n",
      "[Epoch  1/200]: train_loss = 0.9030, validation_loss = 0.8853, RA = 0.2298, BA: 0.5000, CM:(319, 1069, 0, 0)\n",
      "[Epoch  2/200]: train_loss = 0.8925, validation_loss = 0.8738, RA = 0.2298, BA: 0.5000, CM:(319, 1069, 0, 0)\n",
      "[Epoch  3/200]: train_loss = 0.8719, validation_loss = 0.8458, RA = 0.6801, BA: 0.5163, CM:(68, 193, 876, 251)\n",
      "[Epoch  4/200]: train_loss = 0.8349, validation_loss = 0.7986, RA = 0.7695, BA: 0.4995, CM:(0, 1, 1068, 319)\n",
      "[Epoch  5/200]: train_loss = 0.7768, validation_loss = 0.7378, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch  6/200]: train_loss = 0.7254, validation_loss = 0.7069, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch  7/200]: train_loss = 0.7055, validation_loss = 0.6986, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch  8/200]: train_loss = 0.6995, validation_loss = 0.6959, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch  9/200]: train_loss = 0.6971, validation_loss = 0.6948, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 10/200]: train_loss = 0.6959, validation_loss = 0.6942, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 11/200]: train_loss = 0.6952, validation_loss = 0.6939, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 12/200]: train_loss = 0.6948, validation_loss = 0.6937, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 13/200]: train_loss = 0.6945, validation_loss = 0.6936, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 14/200]: train_loss = 0.6942, validation_loss = 0.6935, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 15/200]: train_loss = 0.6941, validation_loss = 0.6934, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 16/200]: train_loss = 0.6939, validation_loss = 0.6933, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 17/200]: train_loss = 0.6938, validation_loss = 0.6933, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 18/200]: train_loss = 0.6938, validation_loss = 0.6933, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 19/200]: train_loss = 0.6937, validation_loss = 0.6933, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 20/200]: train_loss = 0.6936, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 21/200]: train_loss = 0.6936, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 22/200]: train_loss = 0.6935, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 23/200]: train_loss = 0.6935, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 24/200]: train_loss = 0.6935, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 25/200]: train_loss = 0.6935, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 26/200]: train_loss = 0.6934, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 27/200]: train_loss = 0.6934, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 28/200]: train_loss = 0.6934, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 29/200]: train_loss = 0.6934, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 30/200]: train_loss = 0.6934, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 31/200]: train_loss = 0.6934, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 32/200]: train_loss = 0.6933, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 33/200]: train_loss = 0.6933, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 34/200]: train_loss = 0.6933, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 35/200]: train_loss = 0.6933, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 36/200]: train_loss = 0.6933, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 37/200]: train_loss = 0.6933, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 38/200]: train_loss = 0.6933, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 39/200]: train_loss = 0.6933, validation_loss = 0.6932, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 40/200]: train_loss = 0.6933, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 41/200]: train_loss = 0.6933, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 42/200]: train_loss = 0.6933, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 43/200]: train_loss = 0.6933, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 44/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 45/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 46/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 47/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 48/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 49/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 50/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 51/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 52/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 53/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 54/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 55/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 56/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 57/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 58/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 59/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 60/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 61/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 62/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 63/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 64/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 65/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 66/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 67/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 68/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 69/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 70/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 71/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 72/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 73/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 74/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 75/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 76/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 77/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 78/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 79/200]: train_loss = 0.6932, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 80/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 81/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 82/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 83/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 84/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 85/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 86/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 87/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 88/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 89/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 90/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 91/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 92/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 93/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 94/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 95/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 96/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 97/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 98/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 99/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 100/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 101/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 102/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 103/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 104/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 105/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 106/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 107/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 108/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 109/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 110/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 111/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 112/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 113/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 114/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 115/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 116/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 117/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 118/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 119/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 120/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 121/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 122/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 123/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 124/200]: train_loss = 0.6931, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 125/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 126/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 127/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 128/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 129/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 130/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 131/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 132/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 133/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 134/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 135/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 136/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 137/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 138/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 139/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 140/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 141/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 142/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 143/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 144/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 145/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 146/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 147/200]: train_loss = 0.6930, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 148/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 149/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 150/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 151/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 152/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 153/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 154/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 155/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 156/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 157/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 158/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 159/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 160/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 161/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 162/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 163/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 164/200]: train_loss = 0.6929, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 165/200]: train_loss = 0.6928, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 166/200]: train_loss = 0.6928, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 167/200]: train_loss = 0.6928, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 168/200]: train_loss = 0.6928, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 169/200]: train_loss = 0.6927, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 170/200]: train_loss = 0.6927, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 171/200]: train_loss = 0.6927, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 172/200]: train_loss = 0.6927, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 173/200]: train_loss = 0.6926, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 174/200]: train_loss = 0.6926, validation_loss = 0.6931, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 175/200]: train_loss = 0.6925, validation_loss = 0.6930, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 176/200]: train_loss = 0.6925, validation_loss = 0.6930, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 177/200]: train_loss = 0.6924, validation_loss = 0.6929, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 178/200]: train_loss = 0.6923, validation_loss = 0.6926, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 179/200]: train_loss = 0.6923, validation_loss = 0.6921, RA = 0.7702, BA: 0.5000, CM:(0, 0, 1069, 319)\n",
      "[Epoch 180/200]: train_loss = 0.6922, validation_loss = 0.6911, RA = 0.7695, BA: 0.4995, CM:(0, 1, 1068, 319)\n",
      "[Epoch 181/200]: train_loss = 0.6921, validation_loss = 0.6895, RA = 0.7695, BA: 0.4995, CM:(0, 1, 1068, 319)\n",
      "[Epoch 182/200]: train_loss = 0.6920, validation_loss = 0.6875, RA = 0.7695, BA: 0.4995, CM:(0, 1, 1068, 319)\n",
      "[Epoch 183/200]: train_loss = 0.6918, validation_loss = 0.6857, RA = 0.7695, BA: 0.4995, CM:(0, 1, 1068, 319)\n",
      "[Epoch 184/200]: train_loss = 0.6915, validation_loss = 0.6827, RA = 0.7680, BA: 0.4986, CM:(0, 3, 1066, 319)\n",
      "[Epoch 185/200]: train_loss = 0.6907, validation_loss = 0.6771, RA = 0.7680, BA: 0.4997, CM:(1, 4, 1065, 318)\n",
      "[Epoch 186/200]: train_loss = 0.6892, validation_loss = 0.6708, RA = 0.7716, BA: 0.5086, CM:(7, 5, 1064, 312)\n",
      "[Epoch 187/200]: train_loss = 0.6868, validation_loss = 0.6656, RA = 0.7788, BA: 0.5298, CM:(22, 10, 1059, 297)\n",
      "[Epoch 188/200]: train_loss = 0.6833, validation_loss = 0.6602, RA = 0.7824, BA: 0.5508, CM:(39, 22, 1047, 280)\n",
      "[Epoch 189/200]: train_loss = 0.6794, validation_loss = 0.6547, RA = 0.7846, BA: 0.5621, CM:(48, 28, 1041, 271)\n",
      "[Epoch 190/200]: train_loss = 0.6757, validation_loss = 0.6482, RA = 0.7896, BA: 0.5808, CM:(62, 35, 1034, 257)\n",
      "[Epoch 191/200]: train_loss = 0.6724, validation_loss = 0.6407, RA = 0.7932, BA: 0.5930, CM:(71, 39, 1030, 248)\n",
      "[Epoch 192/200]: train_loss = 0.6695, validation_loss = 0.6333, RA = 0.7968, BA: 0.5998, CM:(75, 38, 1031, 244)\n",
      "[Epoch 193/200]: train_loss = 0.6667, validation_loss = 0.6281, RA = 0.8004, BA: 0.6076, CM:(80, 38, 1031, 239)\n",
      "[Epoch 194/200]: train_loss = 0.6642, validation_loss = 0.6248, RA = 0.8055, BA: 0.6175, CM:(86, 37, 1032, 233)\n",
      "[Epoch 195/200]: train_loss = 0.6618, validation_loss = 0.6220, RA = 0.8055, BA: 0.6197, CM:(88, 39, 1030, 231)\n",
      "[Epoch 196/200]: train_loss = 0.6595, validation_loss = 0.6191, RA = 0.8098, BA: 0.6302, CM:(95, 40, 1029, 224)\n",
      "[Epoch 197/200]: train_loss = 0.6573, validation_loss = 0.6172, RA = 0.8098, BA: 0.6357, CM:(100, 45, 1024, 219)\n",
      "[Epoch 198/200]: train_loss = 0.6551, validation_loss = 0.6163, RA = 0.8105, BA: 0.6439, CM:(107, 51, 1018, 212)\n",
      "[Epoch 199/200]: train_loss = 0.6531, validation_loss = 0.6159, RA = 0.8112, BA: 0.6520, CM:(114, 57, 1012, 205)\n",
      "[Epoch 200/200]: train_loss = 0.6512, validation_loss = 0.6157, RA = 0.8098, BA: 0.6533, CM:(116, 61, 1008, 203)\n",
      "(177, 160, 1174, 224)\n",
      "P_A\n",
      "[Epoch  1/200]: train_loss = 0.7059, validation_loss = 0.7024, RA = 0.6527, BA: 0.6238, CM:(737, 469, 169, 13)\n",
      "[Epoch  2/200]: train_loss = 0.6985, validation_loss = 0.6934, RA = 0.5958, BA: 0.6253, CM:(195, 6, 632, 555)\n",
      "[Epoch  3/200]: train_loss = 0.6864, validation_loss = 0.6772, RA = 0.6138, BA: 0.6417, CM:(222, 8, 630, 528)\n",
      "[Epoch  4/200]: train_loss = 0.6676, validation_loss = 0.6529, RA = 0.6837, BA: 0.7051, CM:(330, 19, 619, 420)\n",
      "[Epoch  5/200]: train_loss = 0.6405, validation_loss = 0.6231, RA = 0.7810, BA: 0.7916, CM:(495, 49, 589, 255)\n",
      "[Epoch  6/200]: train_loss = 0.6133, validation_loss = 0.5990, RA = 0.8199, BA: 0.8260, CM:(563, 63, 575, 187)\n",
      "[Epoch  7/200]: train_loss = 0.5920, validation_loss = 0.5823, RA = 0.8278, BA: 0.8322, CM:(583, 72, 566, 167)\n",
      "[Epoch  8/200]: train_loss = 0.5767, validation_loss = 0.5700, RA = 0.8386, BA: 0.8420, CM:(600, 74, 564, 150)\n",
      "[Epoch  9/200]: train_loss = 0.5654, validation_loss = 0.5600, RA = 0.8480, BA: 0.8507, CM:(613, 74, 564, 137)\n",
      "[Epoch 10/200]: train_loss = 0.5567, validation_loss = 0.5513, RA = 0.8494, BA: 0.8520, CM:(615, 74, 564, 135)\n",
      "[Epoch 11/200]: train_loss = 0.5496, validation_loss = 0.5434, RA = 0.8523, BA: 0.8546, CM:(620, 75, 563, 130)\n",
      "[Epoch 12/200]: train_loss = 0.5436, validation_loss = 0.5361, RA = 0.8516, BA: 0.8538, CM:(620, 76, 562, 130)\n",
      "[Epoch 13/200]: train_loss = 0.5385, validation_loss = 0.5295, RA = 0.8523, BA: 0.8537, CM:(627, 82, 556, 123)\n",
      "[Epoch 14/200]: train_loss = 0.5341, validation_loss = 0.5237, RA = 0.8523, BA: 0.8535, CM:(629, 84, 554, 121)\n",
      "[Epoch 15/200]: train_loss = 0.5302, validation_loss = 0.5183, RA = 0.8523, BA: 0.8530, CM:(633, 88, 550, 117)\n",
      "[Epoch 16/200]: train_loss = 0.5268, validation_loss = 0.5135, RA = 0.8509, BA: 0.8516, CM:(632, 89, 549, 118)\n",
      "[Epoch 17/200]: train_loss = 0.5237, validation_loss = 0.5094, RA = 0.8480, BA: 0.8487, CM:(630, 91, 547, 120)\n",
      "[Epoch 18/200]: train_loss = 0.5210, validation_loss = 0.5065, RA = 0.8473, BA: 0.8478, CM:(631, 93, 545, 119)\n",
      "[Epoch 19/200]: train_loss = 0.5185, validation_loss = 0.5045, RA = 0.8480, BA: 0.8486, CM:(631, 92, 546, 119)\n",
      "[Epoch 20/200]: train_loss = 0.5162, validation_loss = 0.5030, RA = 0.8465, BA: 0.8471, CM:(630, 93, 545, 120)\n",
      "[Epoch 21/200]: train_loss = 0.5142, validation_loss = 0.5019, RA = 0.8444, BA: 0.8449, CM:(629, 95, 543, 121)\n",
      "[Epoch 22/200]: train_loss = 0.5123, validation_loss = 0.5011, RA = 0.8437, BA: 0.8440, CM:(630, 97, 541, 120)\n",
      "[Epoch 23/200]: train_loss = 0.5106, validation_loss = 0.5004, RA = 0.8429, BA: 0.8433, CM:(629, 97, 541, 121)\n",
      "[Epoch 24/200]: train_loss = 0.5091, validation_loss = 0.4998, RA = 0.8429, BA: 0.8433, CM:(629, 97, 541, 121)\n",
      "[Epoch 25/200]: train_loss = 0.5076, validation_loss = 0.4992, RA = 0.8415, BA: 0.8416, CM:(630, 100, 538, 120)\n",
      "[Epoch 26/200]: train_loss = 0.5063, validation_loss = 0.4988, RA = 0.8429, BA: 0.8431, CM:(631, 99, 539, 119)\n",
      "[Epoch 27/200]: train_loss = 0.5051, validation_loss = 0.4984, RA = 0.8408, BA: 0.8410, CM:(629, 100, 538, 121)\n",
      "[Epoch 28/200]: train_loss = 0.5041, validation_loss = 0.4980, RA = 0.8393, BA: 0.8395, CM:(628, 101, 537, 122)\n",
      "[Epoch 29/200]: train_loss = 0.5031, validation_loss = 0.4978, RA = 0.8408, BA: 0.8410, CM:(629, 100, 538, 121)\n",
      "[Epoch 30/200]: train_loss = 0.5023, validation_loss = 0.4975, RA = 0.8401, BA: 0.8402, CM:(629, 101, 537, 121)\n",
      "[Epoch 31/200]: train_loss = 0.5016, validation_loss = 0.4973, RA = 0.8401, BA: 0.8402, CM:(629, 101, 537, 121)\n",
      "[Epoch 32/200]: train_loss = 0.5010, validation_loss = 0.4971, RA = 0.8393, BA: 0.8395, CM:(628, 101, 537, 122)\n",
      "[Epoch 33/200]: train_loss = 0.5005, validation_loss = 0.4969, RA = 0.8408, BA: 0.8410, CM:(629, 100, 538, 121)\n",
      "[Epoch 34/200]: train_loss = 0.5000, validation_loss = 0.4968, RA = 0.8408, BA: 0.8410, CM:(629, 100, 538, 121)\n",
      "[Epoch 35/200]: train_loss = 0.4996, validation_loss = 0.4966, RA = 0.8408, BA: 0.8410, CM:(629, 100, 538, 121)\n",
      "[Epoch 36/200]: train_loss = 0.4992, validation_loss = 0.4965, RA = 0.8393, BA: 0.8396, CM:(627, 100, 538, 123)\n",
      "[Epoch 37/200]: train_loss = 0.4988, validation_loss = 0.4964, RA = 0.8386, BA: 0.8388, CM:(627, 101, 537, 123)\n",
      "[Epoch 38/200]: train_loss = 0.4985, validation_loss = 0.4963, RA = 0.8386, BA: 0.8390, CM:(626, 100, 538, 124)\n",
      "[Epoch 39/200]: train_loss = 0.4982, validation_loss = 0.4963, RA = 0.8393, BA: 0.8396, CM:(627, 100, 538, 123)\n",
      "[Epoch 40/200]: train_loss = 0.4980, validation_loss = 0.4962, RA = 0.8401, BA: 0.8403, CM:(628, 100, 538, 122)\n",
      "[Epoch 41/200]: train_loss = 0.4977, validation_loss = 0.4961, RA = 0.8386, BA: 0.8387, CM:(628, 102, 536, 122)\n",
      "[Epoch 42/200]: train_loss = 0.4975, validation_loss = 0.4961, RA = 0.8386, BA: 0.8387, CM:(628, 102, 536, 122)\n",
      "[Epoch 43/200]: train_loss = 0.4973, validation_loss = 0.4960, RA = 0.8393, BA: 0.8394, CM:(629, 102, 536, 121)\n",
      "[Epoch 44/200]: train_loss = 0.4972, validation_loss = 0.4960, RA = 0.8393, BA: 0.8394, CM:(629, 102, 536, 121)\n",
      "[Epoch 45/200]: train_loss = 0.4970, validation_loss = 0.4959, RA = 0.8379, BA: 0.8377, CM:(630, 105, 533, 120)\n",
      "[Epoch 46/200]: train_loss = 0.4969, validation_loss = 0.4959, RA = 0.8379, BA: 0.8374, CM:(633, 108, 530, 117)\n",
      "[Epoch 47/200]: train_loss = 0.4967, validation_loss = 0.4959, RA = 0.8372, BA: 0.8363, CM:(635, 111, 527, 115)\n",
      "[Epoch 48/200]: train_loss = 0.4965, validation_loss = 0.4959, RA = 0.8372, BA: 0.8361, CM:(637, 113, 525, 113)\n",
      "[Epoch 49/200]: train_loss = 0.4964, validation_loss = 0.4958, RA = 0.8365, BA: 0.8353, CM:(637, 114, 524, 113)\n",
      "[Epoch 50/200]: train_loss = 0.4963, validation_loss = 0.4958, RA = 0.8365, BA: 0.8353, CM:(637, 114, 524, 113)\n",
      "[Epoch 51/200]: train_loss = 0.4962, validation_loss = 0.4958, RA = 0.8365, BA: 0.8353, CM:(637, 114, 524, 113)\n",
      "[Epoch 52/200]: train_loss = 0.4961, validation_loss = 0.4957, RA = 0.8357, BA: 0.8344, CM:(638, 116, 522, 112)\n",
      "[Epoch 53/200]: train_loss = 0.4960, validation_loss = 0.4957, RA = 0.8357, BA: 0.8344, CM:(638, 116, 522, 112)\n",
      "[Epoch 54/200]: train_loss = 0.4959, validation_loss = 0.4957, RA = 0.8357, BA: 0.8344, CM:(638, 116, 522, 112)\n",
      "[Epoch 55/200]: train_loss = 0.4958, validation_loss = 0.4957, RA = 0.8357, BA: 0.8344, CM:(638, 116, 522, 112)\n",
      "[Epoch 56/200]: train_loss = 0.4958, validation_loss = 0.4956, RA = 0.8357, BA: 0.8344, CM:(638, 116, 522, 112)\n",
      "[Epoch 57/200]: train_loss = 0.4957, validation_loss = 0.4956, RA = 0.8357, BA: 0.8344, CM:(638, 116, 522, 112)\n",
      "[Epoch 58/200]: train_loss = 0.4957, validation_loss = 0.4956, RA = 0.8357, BA: 0.8344, CM:(638, 116, 522, 112)\n",
      "[Epoch 59/200]: train_loss = 0.4957, validation_loss = 0.4956, RA = 0.8357, BA: 0.8344, CM:(638, 116, 522, 112)\n",
      "[Epoch 60/200]: train_loss = 0.4956, validation_loss = 0.4956, RA = 0.8365, BA: 0.8352, CM:(638, 115, 523, 112)\n",
      "[Epoch 61/200]: train_loss = 0.4956, validation_loss = 0.4955, RA = 0.8372, BA: 0.8361, CM:(637, 113, 525, 113)\n",
      "[Epoch 62/200]: train_loss = 0.4955, validation_loss = 0.4955, RA = 0.8350, BA: 0.8336, CM:(638, 117, 521, 112)\n",
      "[Epoch 63/200]: train_loss = 0.4955, validation_loss = 0.4955, RA = 0.8336, BA: 0.8321, CM:(638, 119, 519, 112)\n",
      "[Epoch 64/200]: train_loss = 0.4954, validation_loss = 0.4955, RA = 0.8336, BA: 0.8321, CM:(638, 119, 519, 112)\n",
      "[Epoch 65/200]: train_loss = 0.4954, validation_loss = 0.4955, RA = 0.8336, BA: 0.8321, CM:(638, 119, 519, 112)\n",
      "[Epoch 66/200]: train_loss = 0.4954, validation_loss = 0.4955, RA = 0.8336, BA: 0.8321, CM:(638, 119, 519, 112)\n",
      "[Epoch 67/200]: train_loss = 0.4954, validation_loss = 0.4955, RA = 0.8336, BA: 0.8321, CM:(638, 119, 519, 112)\n",
      "[Epoch 68/200]: train_loss = 0.4954, validation_loss = 0.4955, RA = 0.8336, BA: 0.8321, CM:(638, 119, 519, 112)\n",
      "[Epoch 69/200]: train_loss = 0.4953, validation_loss = 0.4955, RA = 0.8336, BA: 0.8321, CM:(638, 119, 519, 112)\n",
      "[Epoch 70/200]: train_loss = 0.4953, validation_loss = 0.4955, RA = 0.8336, BA: 0.8321, CM:(638, 119, 519, 112)\n",
      "[Epoch 71/200]: train_loss = 0.4953, validation_loss = 0.4954, RA = 0.8336, BA: 0.8321, CM:(638, 119, 519, 112)\n",
      "[Epoch 72/200]: train_loss = 0.4953, validation_loss = 0.4954, RA = 0.8336, BA: 0.8321, CM:(638, 119, 519, 112)\n",
      "[Epoch 73/200]: train_loss = 0.4953, validation_loss = 0.4954, RA = 0.8343, BA: 0.8329, CM:(638, 118, 520, 112)\n",
      "[Epoch 74/200]: train_loss = 0.4953, validation_loss = 0.4954, RA = 0.8357, BA: 0.8342, CM:(640, 118, 520, 110)\n",
      "[Epoch 75/200]: train_loss = 0.4952, validation_loss = 0.4954, RA = 0.8350, BA: 0.8334, CM:(640, 119, 519, 110)\n",
      "[Epoch 76/200]: train_loss = 0.4952, validation_loss = 0.4954, RA = 0.8343, BA: 0.8327, CM:(639, 119, 519, 111)\n",
      "[Epoch 77/200]: train_loss = 0.4952, validation_loss = 0.4954, RA = 0.8343, BA: 0.8327, CM:(639, 119, 519, 111)\n",
      "[Epoch 78/200]: train_loss = 0.4952, validation_loss = 0.4954, RA = 0.8343, BA: 0.8327, CM:(639, 119, 519, 111)\n",
      "[Epoch 79/200]: train_loss = 0.4951, validation_loss = 0.4954, RA = 0.8343, BA: 0.8327, CM:(639, 119, 519, 111)\n",
      "[Epoch 80/200]: train_loss = 0.4951, validation_loss = 0.4954, RA = 0.8321, BA: 0.8309, CM:(635, 118, 520, 115)\n",
      "[Epoch 81/200]: train_loss = 0.4950, validation_loss = 0.4954, RA = 0.8314, BA: 0.8301, CM:(635, 119, 519, 115)\n",
      "[Epoch 82/200]: train_loss = 0.4950, validation_loss = 0.4954, RA = 0.8343, BA: 0.8327, CM:(639, 119, 519, 111)\n",
      "[Epoch 83/200]: train_loss = 0.4950, validation_loss = 0.4954, RA = 0.8336, BA: 0.8320, CM:(639, 120, 518, 111)\n",
      "[Epoch 84/200]: train_loss = 0.4950, validation_loss = 0.4954, RA = 0.8343, BA: 0.8327, CM:(639, 119, 519, 111)\n",
      "[Epoch 85/200]: train_loss = 0.4950, validation_loss = 0.4954, RA = 0.8343, BA: 0.8327, CM:(639, 119, 519, 111)\n",
      "[Epoch 86/200]: train_loss = 0.4950, validation_loss = 0.4954, RA = 0.8314, BA: 0.8301, CM:(635, 119, 519, 115)\n",
      "[Epoch 87/200]: train_loss = 0.4949, validation_loss = 0.4954, RA = 0.8314, BA: 0.8302, CM:(634, 118, 520, 116)\n",
      "[Epoch 88/200]: train_loss = 0.4949, validation_loss = 0.4954, RA = 0.8329, BA: 0.8312, CM:(639, 121, 517, 111)\n",
      "[Epoch 89/200]: train_loss = 0.4948, validation_loss = 0.4954, RA = 0.8336, BA: 0.8317, CM:(641, 122, 516, 109)\n",
      "[Epoch 90/200]: train_loss = 0.4948, validation_loss = 0.4954, RA = 0.8336, BA: 0.8317, CM:(641, 122, 516, 109)\n",
      "[Epoch 91/200]: train_loss = 0.4948, validation_loss = 0.4954, RA = 0.8321, BA: 0.8305, CM:(638, 121, 517, 112)\n",
      "[Epoch 92/200]: train_loss = 0.4947, validation_loss = 0.4954, RA = 0.8329, BA: 0.8313, CM:(638, 120, 518, 112)\n",
      "[Epoch 93/200]: train_loss = 0.4947, validation_loss = 0.4954, RA = 0.8329, BA: 0.8312, CM:(639, 121, 517, 111)\n",
      "[Epoch 94/200]: train_loss = 0.4947, validation_loss = 0.4954, RA = 0.8329, BA: 0.8312, CM:(639, 121, 517, 111)\n",
      "[Epoch 95/200]: train_loss = 0.4947, validation_loss = 0.4954, RA = 0.8329, BA: 0.8312, CM:(639, 121, 517, 111)\n",
      "[Epoch 96/200]: train_loss = 0.4947, validation_loss = 0.4953, RA = 0.8329, BA: 0.8312, CM:(639, 121, 517, 111)\n",
      "[Epoch 97/200]: train_loss = 0.4947, validation_loss = 0.4953, RA = 0.8329, BA: 0.8312, CM:(639, 121, 517, 111)\n",
      "[Epoch 98/200]: train_loss = 0.4947, validation_loss = 0.4953, RA = 0.8329, BA: 0.8312, CM:(639, 121, 517, 111)\n",
      "[Epoch 99/200]: train_loss = 0.4947, validation_loss = 0.4953, RA = 0.8321, BA: 0.8304, CM:(639, 122, 516, 111)\n",
      "[Epoch 100/200]: train_loss = 0.4947, validation_loss = 0.4953, RA = 0.8321, BA: 0.8304, CM:(639, 122, 516, 111)\n",
      "[Epoch 101/200]: train_loss = 0.4947, validation_loss = 0.4953, RA = 0.8321, BA: 0.8304, CM:(639, 122, 516, 111)\n",
      "[Epoch 102/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8321, BA: 0.8304, CM:(639, 122, 516, 111)\n",
      "[Epoch 103/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8321, BA: 0.8304, CM:(639, 122, 516, 111)\n",
      "[Epoch 104/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8321, BA: 0.8304, CM:(639, 122, 516, 111)\n",
      "[Epoch 105/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8321, BA: 0.8304, CM:(639, 122, 516, 111)\n",
      "[Epoch 106/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8321, BA: 0.8304, CM:(639, 122, 516, 111)\n",
      "[Epoch 107/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8329, BA: 0.8312, CM:(639, 121, 517, 111)\n",
      "[Epoch 108/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8329, BA: 0.8312, CM:(639, 121, 517, 111)\n",
      "[Epoch 109/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8336, BA: 0.8320, CM:(639, 120, 518, 111)\n",
      "[Epoch 110/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8343, BA: 0.8326, CM:(640, 120, 518, 110)\n",
      "[Epoch 111/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8343, BA: 0.8326, CM:(640, 120, 518, 110)\n",
      "[Epoch 112/200]: train_loss = 0.4946, validation_loss = 0.4953, RA = 0.8357, BA: 0.8343, CM:(639, 117, 521, 111)\n",
      "[Epoch 113/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8365, BA: 0.8347, CM:(642, 119, 519, 108)\n",
      "[Epoch 114/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8365, BA: 0.8347, CM:(642, 119, 519, 108)\n",
      "[Epoch 115/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8365, BA: 0.8347, CM:(642, 119, 519, 108)\n",
      "[Epoch 116/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8365, BA: 0.8347, CM:(642, 119, 519, 108)\n",
      "[Epoch 117/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8365, BA: 0.8347, CM:(642, 119, 519, 108)\n",
      "[Epoch 118/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8365, BA: 0.8347, CM:(642, 119, 519, 108)\n",
      "[Epoch 119/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 120/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 121/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 122/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 123/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 124/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 125/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 126/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 127/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 128/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 129/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 130/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 131/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 132/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 133/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 134/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 135/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 136/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 137/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 138/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 139/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 140/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 141/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 142/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 143/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 144/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 145/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 146/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 147/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 148/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 149/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8357, BA: 0.8340, CM:(642, 120, 518, 108)\n",
      "[Epoch 150/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 151/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 152/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 153/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 154/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 155/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 156/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 157/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 158/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 159/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 160/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 161/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 162/200]: train_loss = 0.4945, validation_loss = 0.4953, RA = 0.8285, BA: 0.8259, CM:(644, 132, 506, 106)\n",
      "[Epoch 163/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 164/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8343, BA: 0.8323, CM:(643, 123, 515, 107)\n",
      "[Epoch 165/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8343, BA: 0.8323, CM:(643, 123, 515, 107)\n",
      "[Epoch 166/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8336, BA: 0.8316, CM:(642, 123, 515, 108)\n",
      "[Epoch 167/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8343, BA: 0.8323, CM:(643, 123, 515, 107)\n",
      "[Epoch 168/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8343, BA: 0.8323, CM:(643, 123, 515, 107)\n",
      "[Epoch 169/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 170/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 171/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 172/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 173/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 174/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 175/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 176/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 177/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 178/200]: train_loss = 0.4944, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 179/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 180/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 181/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 182/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 183/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8350, BA: 0.8331, CM:(643, 122, 516, 107)\n",
      "[Epoch 184/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8357, BA: 0.8338, CM:(643, 121, 517, 107)\n",
      "[Epoch 185/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8357, BA: 0.8338, CM:(643, 121, 517, 107)\n",
      "[Epoch 186/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8357, BA: 0.8338, CM:(643, 121, 517, 107)\n",
      "[Epoch 187/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8357, BA: 0.8338, CM:(643, 121, 517, 107)\n",
      "[Epoch 188/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8357, BA: 0.8338, CM:(643, 121, 517, 107)\n",
      "[Epoch 189/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8350, BA: 0.8332, CM:(642, 121, 517, 108)\n",
      "[Epoch 190/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8343, BA: 0.8324, CM:(642, 122, 516, 108)\n",
      "[Epoch 191/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8343, BA: 0.8324, CM:(642, 122, 516, 108)\n",
      "[Epoch 192/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8343, BA: 0.8324, CM:(642, 122, 516, 108)\n",
      "[Epoch 193/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8343, BA: 0.8324, CM:(642, 122, 516, 108)\n",
      "[Epoch 194/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8343, BA: 0.8324, CM:(642, 122, 516, 108)\n",
      "[Epoch 195/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8343, BA: 0.8324, CM:(642, 122, 516, 108)\n",
      "[Epoch 196/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8343, BA: 0.8324, CM:(642, 122, 516, 108)\n",
      "[Epoch 197/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8343, BA: 0.8324, CM:(642, 122, 516, 108)\n",
      "[Epoch 198/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8329, BA: 0.8308, CM:(642, 124, 514, 108)\n",
      "[Epoch 199/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8329, BA: 0.8308, CM:(642, 124, 514, 108)\n",
      "[Epoch 200/200]: train_loss = 0.4943, validation_loss = 0.4953, RA = 0.8329, BA: 0.8308, CM:(642, 124, 514, 108)\n",
      "(764, 213, 583, 175)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>BA</th>\n",
       "      <th>RA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>19</td>\n",
       "      <td>0.653595</td>\n",
       "      <td>0.873199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>39</td>\n",
       "      <td>0.666540</td>\n",
       "      <td>0.697406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>199</td>\n",
       "      <td>0.660728</td>\n",
       "      <td>0.778674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>0.773022</td>\n",
       "      <td>0.776369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Epoch        BA        RA\n",
       "0     O     19  0.653595  0.873199\n",
       "1     C     39  0.666540  0.697406\n",
       "2     E    199  0.660728  0.778674\n",
       "3     A     11  0.773022  0.776369"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bow\n",
    "\n",
    "# # Part 1: Environment Setup\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "# import general_module which locate at my parent directory's child\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from general_module.evaluation import *\n",
    "from general_module.training import *\n",
    "from model_training.loss import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "# # Part 2: Load dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, dimension, feature):\n",
    "        self.dataframe = dataframe\n",
    "        self.dimension = dimension\n",
    "        self.feature = feature\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.feature == \"bow\":\n",
    "            fea = torch.tensor(self.dataframe['text'].values[index])\n",
    "        elif self.feature == \"psycho\":\n",
    "            fea = torch.tensor(self.dataframe['psychofeature'].values[index])\n",
    "        elif self.feature == \"bow+psycho\":\n",
    "            #merge two features\n",
    "            fea = torch.tensor(np.concatenate((self.dataframe['text'].values[0], self.dataframe['psychofeature'].values[0])))\n",
    "\n",
    "        fea = fea.type(torch.FloatTensor)\n",
    "\n",
    "        label = torch.tensor(float(str(self.dataframe[self.dimension].values[index])))\n",
    "        label = label.type(torch.FloatTensor)\n",
    "\n",
    "        return fea, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "\n",
    "# # Part 3 Model Training\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 5)\n",
    "        self.fc2 = nn.Linear(5, 5)\n",
    "        self.fc3 = nn.Linear(5, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "def train(loss_function, epochs, trainloader, validationloader, testloader):\n",
    "\n",
    "    checkpoint = Checkpoint()\n",
    "    # get the input_size from trainloader\n",
    "    in_size= trainloader.dataset[0][0].shape[0]\n",
    "\n",
    "    network = CustomNetwork(input_size=in_size)\n",
    "    network = best_device(network)\n",
    "\n",
    "    model = CustomModel(network)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # a dictionary that store the training loss, validation loss, train_size, validation_size, TP, FP, TN, FN\n",
    "        running_info = {'train_loss':0, 'validation_loss':0, 'train_size':0, 'validation_size':0, 'TP':0, 'FP':0, 'TN':0, 'FN':0}\n",
    "\n",
    "        # set to training mode\n",
    "        network.train(True)\n",
    "\n",
    "        # per epoch training activity\n",
    "        for inputs, labels in trainloader:\n",
    "\n",
    "            # clear all the gradient to 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs,labels = best_device(inputs, labels)\n",
    "\n",
    "            # forward propagation\n",
    "            outs = network(inputs)\n",
    "            outs = outs.view(-1)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = loss_function.forward(inputs=outs, targets=labels)\n",
    "            \n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # update w\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running_info\n",
    "            running_info['train_loss'] += loss.item()*labels.size(0)\n",
    "            running_info['train_size'] += labels.size(0)\n",
    "\n",
    "\n",
    "        # Turn off training mode for reporting validation loss\n",
    "        network.train(False)\n",
    "\n",
    "        # per epoch validation activity\n",
    "        for inputs, labels in validationloader:\n",
    "\n",
    " \n",
    "            inputs,labels = best_device(inputs, labels)\n",
    "\n",
    "            # forward propagation\n",
    "            outs = network(inputs)\n",
    "            outs = outs.view(-1)\n",
    "\n",
    "            # update running_info\n",
    "            running_info['validation_loss'] += loss.item()*labels.size(0)\n",
    "            running_info['validation_size'] += labels.size(0)\n",
    "\n",
    "            preds = (outs > 0.5).type(torch.FloatTensor)\n",
    "            tp,fp,tn,fn = e_confusion_matrix(preds,labels)\n",
    "            running_info['TP'] += tp\n",
    "            running_info['FP'] += fp\n",
    "            running_info['TN'] += tn\n",
    "            running_info['FN'] += fn\n",
    "        \n",
    "        train_loss = running_info['train_loss']/running_info['train_size']\n",
    "        validation_loss = running_info['validation_loss']/running_info['validation_size']\n",
    "\n",
    " \n",
    "        confusion_matrix=(running_info['TP'],running_info['FP'],running_info['TN'],running_info['FN'])\n",
    "        regular_accuracy,balanced_accuracy = e_accuracy(confusion_matrix)\n",
    "\n",
    "        print(f'[Epoch {e + 1:2d}/{epochs:d}]: train_loss = {train_loss:.4f}, validation_loss = {validation_loss:.4f}, RA = {regular_accuracy:.4f}, BA: {balanced_accuracy:.4f}, CM:{confusion_matrix}')\n",
    "\n",
    "        model.update(network, epochs = e+1, ba = balanced_accuracy, ra=regular_accuracy)\n",
    "\n",
    "        checkpoint.add(network.state_dict(),optimizer.state_dict())\n",
    "\n",
    "    m_dict = checkpoint.get(model.getOptEpoch())\n",
    "    network.load_state_dict(m_dict)\n",
    "\n",
    "    network.eval()\n",
    "\n",
    "    running_info = {'test_loss':0, 'test_size':0, 'TP':0, 'FP':0, 'TN':0, 'FN':0}\n",
    "     # per epoch test activity\n",
    "    for inputs, labels in testloader:\n",
    "        inputs,labels = best_device(inputs, labels)\n",
    "\n",
    "        # forward propagation\n",
    "        outs = network(inputs)\n",
    "        outs = outs.view(-1)\n",
    "\n",
    "        # update running_info\n",
    "        running_info['test_loss'] += loss.item()*labels.size(0)\n",
    "        running_info['test_size'] += labels.size(0)\n",
    "\n",
    "        preds = (outs > 0.5).type(torch.FloatTensor)\n",
    "        tp,fp,tn,fn = e_confusion_matrix(preds,labels)\n",
    "        running_info['TP'] += tp\n",
    "        running_info['FP'] += fp\n",
    "        running_info['TN'] += tn\n",
    "        running_info['FN'] += fn\n",
    "    \n",
    "    confusion_matrix=(running_info['TP'],running_info['FP'],running_info['TN'],running_info['FN'])\n",
    "    regular_accuracy,balanced_accuracy = e_accuracy(confusion_matrix)\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    model.override(network=network, ba = balanced_accuracy, ra=regular_accuracy)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# # Execution\n",
    "\n",
    "def run(config):\n",
    "    trainset_dataframe = extract(\"../dataset/personality/\"+config[\"dataset\"]+\"_train.pickle\")\n",
    "    validationset_dataframe = extract(\"../dataset/personality/\"+config[\"dataset\"]+\"_validation.pickle\")\n",
    "    testset_dataframe = extract(\"../dataset/personality/\"+config[\"dataset\"]+\"_test.pickle\")\n",
    "    \n",
    "\n",
    "    if config[\"dataset\"] ==\"mbti\":\n",
    "        models = {\"O\":None, \"C\":None, \"E\":None, \"A\":None}\n",
    "    else:\n",
    "        raise Exception(\"dataset name not found\")\n",
    "\n",
    "    if config[\"feature\"] != \"bow+psycho\" and config[\"feature\"] != \"bow\" and config[\"feature\"] != \"psycho\":\n",
    "        raise ValueError(\"feature must be one of bow, psycho, bow+psycho\")\n",
    "    \n",
    "\n",
    "\n",
    "    # train the model on different personality dimension using for loop on the key of the dictionary\n",
    "    for dimension in models.keys():\n",
    "\n",
    "        loss_function = BCE()\n",
    "\n",
    "        print(\"P_\"+dimension)\n",
    "\n",
    "        custom_trainset = CustomDataset(dataframe=trainset_dataframe,dimension=dimension, feature=config[\"feature\"])\n",
    "        custom_validationset = CustomDataset(dataframe=validationset_dataframe,dimension=dimension, feature=config[\"feature\"])\n",
    "        custom_textset = CustomDataset(dataframe=testset_dataframe,dimension=dimension, feature=config[\"feature\"])\n",
    "\n",
    "        batch_size = 128\n",
    "        trainloader = DataLoader(custom_trainset, batch_size=batch_size, shuffle=False)\n",
    "        validationloader = DataLoader(custom_validationset, batch_size=batch_size, shuffle=False)\n",
    "        testloader = DataLoader(custom_textset,batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = train(loss_function = loss_function, epochs= config[\"epochs\"], trainloader=trainloader,testloader = testloader, validationloader=validationloader)\n",
    "        \n",
    "        models[dimension] = model\n",
    "\n",
    "    return models\n",
    "\n",
    "config ={\n",
    "    \"dataset\":\"mbti\", #mbti, essays\n",
    "    \"feature\":\"bow\", #bow, psycho, bow+psycho\n",
    "    \"epochs\":200 #any\n",
    "}\n",
    "\n",
    "models= run(config=config)\n",
    "\n",
    "print_results(models)\n",
    "\n",
    "# save model in personality_model folder \n",
    "for model_key in models.keys():\n",
    "    torch.save(models[model_key].network, \"../personality_model/\"+model_key+\".pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phdwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
