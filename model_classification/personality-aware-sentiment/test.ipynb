{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow\n",
    "\n",
    "# # Part 1: Environment Setup\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "# import general_module which locate at my parent directory's child\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from general_module.evaluation import *\n",
    "from general_module.training import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clearing(data):\n",
    "    cleaned_text=[]\n",
    "    for sentence in data:\n",
    "        sentence=sentence.lower()\n",
    "        # removing links from text data\n",
    "        sentence=re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',sentence)\n",
    "    \n",
    "        # removing other symbols\n",
    "        sentence=re.sub('[^0-9a-z]',' ',sentence)\n",
    "        cleaned_text.append(sentence)\n",
    "    return cleaned_text\n",
    "\n",
    "def expand(np_data):\n",
    "    temp=[]\n",
    "    for i in range(len(np_data)):\n",
    "        temp.append(np.array(np_data[i]))\n",
    "    return temp\n",
    "\n",
    "class Lemmatizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, sentence):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in sentence.split() if len(word)>2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sen = self.dataframe[\"content\"].values[index]\n",
    "        aware = np.array(self.dataframe[[\"O\",\"C\",\"E\",\"A\"]].values[index],np.float64)\n",
    "        fea = np.concatenate((sen,aware),axis=0)\n",
    "\n",
    "        fea = torch.tensor(fea, dtype=torch.float64)\n",
    "        fea = fea.type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "        label = torch.tensor(float(str(self.dataframe[\"label\"].values[index])), dtype=torch.float64)\n",
    "        label = label.type(torch.FloatTensor)\n",
    "\n",
    "        return fea, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Part 3 Model Training\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(5004, 100)\n",
    "        self.fc2 = nn.Linear(100, 5)\n",
    "        self.fc3 = nn.Linear(5, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(epochs, trainloader, validationloader, testloader):\n",
    "\n",
    "    checkpoint = Checkpoint()\n",
    "\n",
    "\n",
    "    network = CustomNetwork()\n",
    "    network = best_device(network)\n",
    "    loss_function = nn.BCELoss()\n",
    "    model = CustomModel(network)\n",
    "\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=0.0005)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # a dictionary that store the training loss, validation loss, train_size, validation_size, TP, FP, TN, FN\n",
    "        running_info = {'train_loss':0, 'validation_loss':0, 'train_size':0, 'validation_size':0, 'TP':0, 'FP':0, 'TN':0, 'FN':0}\n",
    "\n",
    "        # set to training mode\n",
    "        network.train(True)\n",
    "\n",
    "        # per epoch training activity\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs,labels = best_device(inputs, labels)\n",
    "\n",
    "            # clear all the gradient to 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward propagation\n",
    "            outs = network(inputs)\n",
    "            outs = outs.view(-1)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = loss_function(outs,labels)\n",
    "            \n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # update w\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running_info\n",
    "            running_info['train_loss'] += loss.item()*labels.size(0)\n",
    "            running_info['train_size'] += labels.size(0)\n",
    "\n",
    "\n",
    "        # Turn off training mode for reporting validation loss\n",
    "        network.train(False)\n",
    "\n",
    "        # per epoch validation activity\n",
    "        for inputs, labels in validationloader:\n",
    "\n",
    " \n",
    "            inputs,labels = best_device(inputs, labels)\n",
    "\n",
    "            # forward propagation\n",
    "            outs = network(inputs)\n",
    "            outs = outs.view(-1)\n",
    "            \n",
    "            loss = loss_function(outs,labels)\n",
    "\n",
    "            # update running_info\n",
    "            running_info['validation_loss'] += loss.item()*labels.size(0)\n",
    "            running_info['validation_size'] += labels.size(0)\n",
    "\n",
    "            preds = (outs > 0.5).type(torch.FloatTensor)\n",
    "            tp,fp,tn,fn = e_confusion_matrix(preds,labels)\n",
    "            running_info['TP'] += tp\n",
    "            running_info['FP'] += fp\n",
    "            running_info['TN'] += tn\n",
    "            running_info['FN'] += fn\n",
    "\n",
    "        \n",
    "        train_loss = running_info['train_loss']/running_info['train_size']\n",
    "        validation_loss = running_info['validation_loss']/running_info['validation_size']\n",
    "        confusion_matrix=(running_info['TP'],running_info['FP'],running_info['TN'],running_info['FN'])\n",
    "        regular_accuracy,balanced_accuracy = e_accuracy(confusion_matrix)\n",
    "\n",
    "        print(f'[Epoch {e + 1:2d}/{epochs:d}]: train_loss = {train_loss:.4f}, validation_loss = {validation_loss:.4f}, RA = {regular_accuracy:.4f}, BA: {balanced_accuracy:.4f}, CM:{confusion_matrix}')\n",
    "\n",
    "        model.update(network, epochs = e+1, ba = balanced_accuracy, ra=regular_accuracy)\n",
    "\n",
    "        checkpoint.add(network.state_dict(),optimizer.state_dict())\n",
    "\n",
    "    m_dict = checkpoint.get(model.getOptEpoch())\n",
    "    network.load_state_dict(m_dict)\n",
    "\n",
    "    network.eval()\n",
    "\n",
    "\n",
    "    running_info = {'test_loss':0, 'test_size':0, 'TP':0, 'FP':0, 'TN':0, 'FN':0}\n",
    "     # per epoch test activity\n",
    "    for inputs, labels in testloader:\n",
    "        inputs,labels = best_device(inputs, labels)\n",
    "\n",
    "        # forward propagation\n",
    "        outs = network(inputs)\n",
    "        outs = outs.view(-1)\n",
    "\n",
    "        # update running_info\n",
    "        running_info['test_loss'] += loss.item()*labels.size(0)\n",
    "        running_info['test_size'] += labels.size(0)\n",
    "\n",
    "        preds = (outs > 0.5).type(torch.FloatTensor)\n",
    "        tp,fp,tn,fn = e_confusion_matrix(preds,labels)\n",
    "        running_info['TP'] += tp\n",
    "        running_info['FP'] += fp\n",
    "        running_info['TN'] += tn\n",
    "        running_info['FN'] += fn\n",
    "    \n",
    "    confusion_matrix=(running_info['TP'],running_info['FP'],running_info['TN'],running_info['FN'])\n",
    "    regular_accuracy,balanced_accuracy = e_accuracy(confusion_matrix)\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    model.override(network=network, ba = balanced_accuracy, ra=regular_accuracy)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = extract(\"../../corpus/personality-aware-sentiment/bow-movie-review.pickle\")\n",
    "trainset, testset = train_test_split(dataset, test_size=0.2, random_state=42, stratify=dataset.label)\n",
    "trainset.content = clearing(trainset.content)\n",
    "vectorizer=TfidfVectorizer(max_features=5000,stop_words='english',tokenizer=Lemmatizer())\n",
    "vectorizer.fit(trainset.content)\n",
    "\n",
    "trainset.content=expand(vectorizer.transform(trainset.content).toarray())\n",
    "testset.content=expand(vectorizer.transform(testset.content).toarray())\n",
    "\n",
    "trainset, validationset = train_test_split(trainset, test_size=0.2, random_state=42, stratify=trainset.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "custom_trainset = CustomDataset(dataframe=trainset)\n",
    "custom_validationset = CustomDataset(dataframe=validationset)\n",
    "custom_textset = CustomDataset(dataframe=testset)\n",
    "batch_size = 32\n",
    "trainloader = DataLoader(custom_trainset, batch_size=batch_size, shuffle=False)\n",
    "validationloader = DataLoader(custom_validationset, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(custom_textset,batch_size=batch_size, shuffle=False)\n",
    "model = train(epochs = 20, trainloader=trainloader,testloader = testloader, validationloader=validationloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "custom_trainset = CustomDataset(dataframe=trainset)\n",
    "custom_validationset = CustomDataset(dataframe=validationset)\n",
    "custom_textset = CustomDataset(dataframe=testset)\n",
    "batch_size = 32\n",
    "trainloader = DataLoader(custom_trainset, batch_size=batch_size, shuffle=False)\n",
    "validationloader = DataLoader(custom_validationset, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(custom_textset,batch_size=batch_size, shuffle=False)\n",
    "model = train(epochs = 20, trainloader=trainloader,testloader = testloader, validationloader=validationloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "custom_trainset = CustomDataset(dataframe=trainset)\n",
    "custom_validationset = CustomDataset(dataframe=validationset)\n",
    "custom_textset = CustomDataset(dataframe=testset)\n",
    "batch_size = 32\n",
    "trainloader = DataLoader(custom_trainset, batch_size=batch_size, shuffle=False)\n",
    "validationloader = DataLoader(custom_validationset, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(custom_textset,batch_size=batch_size, shuffle=False)\n",
    "model = train(epochs = 20, trainloader=trainloader,testloader = testloader, validationloader=validationloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phdwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
